{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13900b6dc92e4e599169f89b4254a351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32da7a0dfc88443fb1f2406d62b87e77",
              "IPY_MODEL_32a2195a9783432f91755842cff3a12e",
              "IPY_MODEL_d6941731b17446f88513257bb15e7998"
            ],
            "layout": "IPY_MODEL_66c856bd16a345f58d65b1e5b338e148"
          }
        },
        "32da7a0dfc88443fb1f2406d62b87e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c53c7e6c6c6142b4a6fa946424d0265e",
            "placeholder": "​",
            "style": "IPY_MODEL_c7430be53b4f428e9141d4c4ad3b7f3c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "32a2195a9783432f91755842cff3a12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a78fc9dcb642b6a66a044291375656",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51ae85a97fa54c9ea04d3ab1f1247cc2",
            "value": 4
          }
        },
        "d6941731b17446f88513257bb15e7998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e625429b3d542faab874f1b717786a6",
            "placeholder": "​",
            "style": "IPY_MODEL_acb2615513884286a4673c1a31b91be9",
            "value": " 4/4 [00:08&lt;00:00,  1.95s/it]"
          }
        },
        "66c856bd16a345f58d65b1e5b338e148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c53c7e6c6c6142b4a6fa946424d0265e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7430be53b4f428e9141d4c4ad3b7f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8a78fc9dcb642b6a66a044291375656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ae85a97fa54c9ea04d3ab1f1247cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e625429b3d542faab874f1b717786a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acb2615513884286a4673c1a31b91be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c985971cac242ca9b205736d3990a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56cfd9e819bf4e088b1ed74063406aea",
              "IPY_MODEL_66bc806d3dc44d86b1278fd76566e3fb",
              "IPY_MODEL_3722a7fd63ab4238a20b548fa943d16b"
            ],
            "layout": "IPY_MODEL_66dc2f85dddb4c8ebc276008e1906a93"
          }
        },
        "56cfd9e819bf4e088b1ed74063406aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaf1cfd9ec1f4defb15ffa030d7a1e52",
            "placeholder": "​",
            "style": "IPY_MODEL_81c794f698b54b018f503ed222970377",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "66bc806d3dc44d86b1278fd76566e3fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0af758fa77d146d988cf542a3001b0b4",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e33e03b165ad4a01bcdf5e02822ab54c",
            "value": 4
          }
        },
        "3722a7fd63ab4238a20b548fa943d16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03bd934e23be47c7ac3ddc40f0ed51f2",
            "placeholder": "​",
            "style": "IPY_MODEL_160f20777805408eb9f56663f6f61637",
            "value": " 4/4 [00:01&lt;00:00,  4.67it/s]"
          }
        },
        "66dc2f85dddb4c8ebc276008e1906a93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf1cfd9ec1f4defb15ffa030d7a1e52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81c794f698b54b018f503ed222970377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0af758fa77d146d988cf542a3001b0b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e33e03b165ad4a01bcdf5e02822ab54c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03bd934e23be47c7ac3ddc40f0ed51f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "160f20777805408eb9f56663f6f61637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d93e6295e1840ef97fe9d0311fcf354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_058625bcc90446ecaf1331178a84f8f0",
              "IPY_MODEL_04312be319d344fdbb79f2deda76ecf4",
              "IPY_MODEL_363afabbd9d24f37b3c4401f5c3edaa7"
            ],
            "layout": "IPY_MODEL_8ab6bca18cc245f7b733d98fe744eb2e"
          }
        },
        "058625bcc90446ecaf1331178a84f8f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_563be06d6b1642c88e5c23c8afa0d162",
            "placeholder": "​",
            "style": "IPY_MODEL_a9b5728b26924d6191ef38de088f5bf0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "04312be319d344fdbb79f2deda76ecf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a9e11fdd5c44dfeb970f16478263e77",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6f8593f4400410abb62164a6c4989af",
            "value": 4
          }
        },
        "363afabbd9d24f37b3c4401f5c3edaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e575a0213084eb1aef068d382c6d7dc",
            "placeholder": "​",
            "style": "IPY_MODEL_0baad327934e47a5aa3d6ea1f52e36c4",
            "value": " 4/4 [00:53&lt;00:00,  9.41s/it]"
          }
        },
        "8ab6bca18cc245f7b733d98fe744eb2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "563be06d6b1642c88e5c23c8afa0d162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b5728b26924d6191ef38de088f5bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a9e11fdd5c44dfeb970f16478263e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6f8593f4400410abb62164a6c4989af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e575a0213084eb1aef068d382c6d7dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0baad327934e47a5aa3d6ea1f52e36c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa0cf9f62dda4a30a2c1fa3745af42fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d10e594c8de46819cc500b05b8f7148",
              "IPY_MODEL_206e1c52566c44b398d73a70c80bf698",
              "IPY_MODEL_8710bb36e0c1458583acf93cbf9b9d7b"
            ],
            "layout": "IPY_MODEL_da1f49c6fd0a445ea4dfe3afbb0b88d8"
          }
        },
        "3d10e594c8de46819cc500b05b8f7148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ae40ed01a9f4aba96e317eceb8fc384",
            "placeholder": "​",
            "style": "IPY_MODEL_2a983e2e90e743079117cb4c5aa0b0e6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "206e1c52566c44b398d73a70c80bf698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63929bb31464ec7b012b507791cbf61",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddeef9a671644eb59abebb4b81742c37",
            "value": 4
          }
        },
        "8710bb36e0c1458583acf93cbf9b9d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c986fd050dc4491a652538be2a48b2c",
            "placeholder": "​",
            "style": "IPY_MODEL_81b1b211447e41f48059d52eeaa6230e",
            "value": " 4/4 [00:05&lt;00:00,  1.20s/it]"
          }
        },
        "da1f49c6fd0a445ea4dfe3afbb0b88d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae40ed01a9f4aba96e317eceb8fc384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a983e2e90e743079117cb4c5aa0b0e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d63929bb31464ec7b012b507791cbf61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddeef9a671644eb59abebb4b81742c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c986fd050dc4491a652538be2a48b2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81b1b211447e41f48059d52eeaa6230e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky97-0-oQcO3",
        "outputId": "73ceccae-99b5-484c-87c6-7f5392854dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/MyDrive/powershell-sentinel-main.zip\n",
            "1fbaf135538551d92f958d39f8eefba66f855a44\n",
            "   creating: /content/powershell-sentinel-main/\n",
            "  inflating: /content/powershell-sentinel-main/.gitignore  \n",
            "  inflating: /content/powershell-sentinel-main/README.md  \n",
            "   creating: /content/powershell-sentinel-main/data/\n",
            "   creating: /content/powershell-sentinel-main/data/generated/\n",
            "  inflating: /content/powershell-sentinel-main/data/generated/training_data_v0.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/generated/training_data_v0_clean.json  \n",
            "   creating: /content/powershell-sentinel-main/data/interim/\n",
            "   creating: /content/powershell-sentinel-main/data/interim/curating_logs/\n",
            "  inflating: /content/powershell-sentinel-main/data/interim/curating_logs/uncurated_for_review.json  \n",
            "   creating: /content/powershell-sentinel-main/data/interim/delta_logs/\n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-001.json  \n",
            " extracting: /content/powershell-sentinel-main/data/interim/delta_logs/PS-002.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-003.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-004.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-005.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-006.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-007.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-008.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-009.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-010.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-011.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-012.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-013.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-014.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-015.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-016.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-017.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-018.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-019.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-020.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-021.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-022.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-023.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-024.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-025.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-026.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-027.json  \n",
            " extracting: /content/powershell-sentinel-main/data/interim/delta_logs/PS-028.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-029.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-030.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-031.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-032.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-033.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-034.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-035.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-036.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-037.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-038.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-039.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-040.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-041.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-042.json  \n",
            " extracting: /content/powershell-sentinel-main/data/interim/delta_logs/PS-043.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-044.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-045.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-046.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-047.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-048.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-049.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/interim/delta_logs/PS-050.json  \n",
            "   creating: /content/powershell-sentinel-main/data/interim/parsing_logs/\n",
            "  inflating: /content/powershell-sentinel-main/data/interim/parsing_logs/unparsed_for_review.json  \n",
            "   creating: /content/powershell-sentinel-main/data/sets/\n",
            "  inflating: /content/powershell-sentinel-main/data/sets/test_set_v0.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/sets/training_set_v0.json  \n",
            "   creating: /content/powershell-sentinel-main/data/source/\n",
            "  inflating: /content/powershell-sentinel-main/data/source/mitre_ttp_library.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/source/parsing_rules.json  \n",
            "  inflating: /content/powershell-sentinel-main/data/source/primitives_library.json  \n",
            "   creating: /content/powershell-sentinel-main/powershell_sentinel.egg-info/\n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel.egg-info/PKG-INFO  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel.egg-info/SOURCES.txt  \n",
            " extracting: /content/powershell-sentinel-main/powershell_sentinel.egg-info/dependency_links.txt  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel.egg-info/entry_points.txt  \n",
            " extracting: /content/powershell-sentinel-main/powershell_sentinel.egg-info/top_level.txt  \n",
            "   creating: /content/powershell-sentinel-main/powershell_sentinel/\n",
            " extracting: /content/powershell-sentinel-main/powershell_sentinel/__init__.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/evaluate.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/lab_connector.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/main_data_factory.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/models.py  \n",
            "   creating: /content/powershell-sentinel-main/powershell_sentinel/modules/\n",
            " extracting: /content/powershell-sentinel-main/powershell_sentinel/modules/__init__.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/modules/obfuscator.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/modules/recommendation_engine.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/modules/rule_formatter.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/modules/snapshot_differ.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/modules/statistics_calculator.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/primitives_manager.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/sentinel_toolkit.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/train.py  \n",
            "   creating: /content/powershell-sentinel-main/powershell_sentinel/utils/\n",
            " extracting: /content/powershell-sentinel-main/powershell_sentinel/utils/__init__.py  \n",
            "  inflating: /content/powershell-sentinel-main/powershell_sentinel/utils/metrics.py  \n",
            "  inflating: /content/powershell-sentinel-main/practitioner_package.zip  \n",
            "  inflating: /content/powershell-sentinel-main/requirements.txt  \n",
            "   creating: /content/powershell-sentinel-main/scripts/\n",
            " extracting: /content/powershell-sentinel-main/scripts/__init__.py  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/analyze_dataset_uniqueness.py  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/deduplicate_dataset.py  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/evaluate_pipeline.py  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/generate_dissertation_visuals.py  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/partition_dataset.py  \n",
            "   creating: /content/powershell-sentinel-main/scripts/prompt_engineering/\n",
            "  inflating: /content/powershell-sentinel-main/scripts/prompt_engineering/dataset_prompt_A_Direct.json  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/prompt_engineering/dataset_prompt_B_RolePlay.json  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/prompt_engineering/dataset_prompt_C_Detailed.json  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/prompt_engineering/evaluate_prompts.py  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/prompt_engineering/mini_train.json  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/prompt_engineering/mini_val.json  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/prompt_engineering/prompt_formatter.py  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/prompt_engineering/run_prompt_experiments.psl  \n",
            "  inflating: /content/powershell-sentinel-main/scripts/verify_lab_config.py  \n",
            "  inflating: /content/powershell-sentinel-main/setup.py  \n",
            "  inflating: /content/powershell-sentinel-main/structure_check.txt  \n",
            "   creating: /content/powershell-sentinel-main/tests/\n",
            " extracting: /content/powershell-sentinel-main/tests/__init__.py  \n",
            "   creating: /content/powershell-sentinel-main/tests/modules/\n",
            " extracting: /content/powershell-sentinel-main/tests/modules/__init__.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/modules/test_obfuscator.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/modules/test_recommendation_engine.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/modules/test_rule_formatter.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/modules/test_snapshot_differ.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/modules/test_statistics_calculator.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_cli_logic.py  \n",
            "   creating: /content/powershell-sentinel-main/tests/test_data/\n",
            "  inflating: /content/powershell-sentinel-main/tests/test_data/expert_ground_truth.json  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_data/test_cli_lookup.json  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_data_preparation_scripts.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_evaluate_pipeline.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_integration_blackhole.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_integration_high_volume.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_integration_lab_connection.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_lab_connection.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_main_data_factory.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_master_pipeline_smoke.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_metadata_accuracy.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_mlops_metrics.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_primitives_manager_workflow.py  \n",
            "  inflating: /content/powershell-sentinel-main/tests/test_recommender_accuracy.py  \n",
            "   creating: /content/powershell-sentinel-main/visuals/\n",
            "   creating: /content/powershell-sentinel-main/visuals/tables/\n",
            "  inflating: /content/powershell-sentinel-main/visuals/tables/table_4_2_run_summary.md  \n",
            "  inflating: /content/powershell-sentinel-main/visuals/tables/table_4_3_primitive_failures.md  \n",
            "  inflating: /content/powershell-sentinel-main/visuals/tables/table_4_4_technique_failures.md  \n",
            "  inflating: /content/powershell-sentinel-main/visuals/tables/table_4_5_obfuscation_bias.md  \n",
            "  inflating: /content/powershell-sentinel-main/visuals/tables/table_4_6_inner_variety.md  \n",
            "  inflating: /content/powershell-sentinel-main/visuals/tables/table_5_1_partition_summary.md  \n",
            "  inflating: /content/powershell-sentinel-main/visuals/tables/table_5_2_model_performance.md  \n",
            "/content/powershell-sentinel-main\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Colab Access` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Colab Access`\n",
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.19.1\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate==0.30.1\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting peft==0.10.0\n",
            "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl==0.8.6\n",
            "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting bitsandbytes==0.43.1\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Collecting triton\n",
            "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.19.1)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.70.16)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.1)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (3.12.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.30.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.30.1) (2.6.0+cu124)\n",
            "Collecting tyro>=0.5.11 (from trl==0.8.6)\n",
            "  Downloading tyro-0.9.28-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton) (75.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.8.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.30.1) (1.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (4.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.1) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.30.1) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.28-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, pyarrow-hotfix, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, tokenizers, nvidia-cusolver-cu12, transformers, datasets, bitsandbytes, accelerate, trl, peft\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.4\n",
            "    Uninstalling tokenizers-0.21.4:\n",
            "      Successfully uninstalled tokenizers-0.21.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.55.0\n",
            "    Uninstalling transformers-4.55.0:\n",
            "      Successfully uninstalled transformers-4.55.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.9.0\n",
            "    Uninstalling accelerate-1.9.0:\n",
            "      Successfully uninstalled accelerate-1.9.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.0\n",
            "    Uninstalling peft-0.17.0:\n",
            "      Successfully uninstalled peft-0.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 bitsandbytes-0.43.1 datasets-2.19.1 fsspec-2024.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 peft-0.10.0 pyarrow-hotfix-0.7 shtab-1.7.2 tokenizers-0.19.1 transformers-4.41.2 trl-0.8.6 tyro-0.9.28\n",
            "\n",
            "\n",
            "✅ ✅ ✅ MASTER SETUP COMPLETE. You are ready to train. ✅ ✅ ✅\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Mount Drive & Set Up Project ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "!unzip -o /content/drive/MyDrive/powershell-sentinel-main.zip -d /content/\n",
        "%cd /content/powershell-sentinel-main\n",
        "\n",
        "# --- 2. Authenticate with Hugging Face ---\n",
        "# This step is now CRITICAL for downloading Llama 3\n",
        "!huggingface-cli login\n",
        "# Paste your Hugging Face 'read' token when prompted\n",
        "\n",
        "# --- 3. Definitive Library Installation ---\n",
        "# This is the final, corrected installation command for a stable A100 environment.\n",
        "# It includes 'triton', a critical dependency for bitsandbytes.\n",
        "!pip install -U \"transformers==4.41.2\" \"datasets==2.19.1\" \"accelerate==0.30.1\" \"peft==0.10.0\" \"trl==0.8.6\" \"bitsandbytes==0.43.1\" \"triton\"\n",
        "\n",
        "print(\"\\n\\n✅ ✅ ✅ MASTER SETUP COMPLETE. You are ready to train. ✅ ✅ ✅\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Definitive Library Installation ---\n",
        "# This is the final, corrected installation command for a stable A100 environment.\n",
        "# It forcefully uninstalls conflicting libraries and pins Triton to a known-good version.\n",
        "!pip uninstall bitsandbytes triton -y\n",
        "!pip install -U \"transformers==4.41.2\" \"datasets==2.19.1\" \"accelerate==0.30.1\" \"peft==0.10.0\" \"trl==0.8.6\" \"bitsandbytes==0.43.1\" \"triton==2.3.1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD9Fe8ZBRlZD",
        "outputId": "9c2ffc22-ac60-473d-f7e5-cb5a6ce3ea03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: bitsandbytes 0.43.1\n",
            "Uninstalling bitsandbytes-0.43.1:\n",
            "  Successfully uninstalled bitsandbytes-0.43.1\n",
            "Found existing installation: triton 3.2.0\n",
            "Uninstalling triton-3.2.0:\n",
            "  Successfully uninstalled triton-3.2.0\n",
            "Requirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.11/dist-packages (4.41.2)\n",
            "Requirement already satisfied: datasets==2.19.1 in /usr/local/lib/python3.11/dist-packages (2.19.1)\n",
            "Requirement already satisfied: accelerate==0.30.1 in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: trl==0.8.6 in /usr/local/lib/python3.11/dist-packages (0.8.6)\n",
            "Collecting bitsandbytes==0.43.1\n",
            "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting triton==2.3.1\n",
            "  Downloading triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.1) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (3.12.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.30.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.30.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.11/dist-packages (from trl==0.8.6) (0.9.28)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.8.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (12.4.127)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch>=1.10.0 (from accelerate==0.30.1)\n",
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting sympy>=1.13.3 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting torch>=1.10.0 (from accelerate==0.30.1)\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torch>=1.10.0 (from accelerate==0.30.1)\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (1.13.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch>=1.10.0 (from accelerate==0.30.1)\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Downloading torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate==0.30.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (4.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.1) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.30.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==0.30.1) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n",
            "Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "Downloading triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl (779.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.43.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.1 triton-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. THE SMOKE TEST ---\n",
        "import json\n",
        "\n",
        "print(\"--- Step A: Creating Smoke Test Data ---\")\n",
        "# Create a tiny 3-sample training and validation set\n",
        "full_data_path = '/content/powershell-sentinel-main/data/generated/training_data_v0_clean.json'\n",
        "smoke_train_path = '/content/powershell-sentinel-main/data/generated/smoke_test_train.json'\n",
        "smoke_val_path = '/content/powershell-sentinel-main/scripts/prompt_engineering/smoke_test_val.json'\n",
        "\n",
        "with open(full_data_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "with open(smoke_train_path, 'w') as f:\n",
        "    json.dump(data[:3], f)\n",
        "with open(smoke_val_path, 'w') as f:\n",
        "    json.dump(data[3:6], f)\n",
        "\n",
        "print(\"--- Step B: Running a 10-Step Smoke Test Training ---\")\n",
        "!python -m powershell_sentinel.train \\\n",
        "    --model_name meta-llama/Meta-Llama-3-8B-Instruct \\\n",
        "    --train_dataset {smoke_train_path} \\\n",
        "    --preflight_train_dataset {smoke_train_path} \\\n",
        "    --output_dir models/smoke_test_model \\\n",
        "    --test_dataset data/sets/test_set_v0.json \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --lora_rank 16 \\\n",
        "    --max_steps 10\n",
        "\n",
        "print(\"\\n--- Step C: Running a 3-Sample Smoke Test Evaluation ---\")\n",
        "!python -m powershell_sentinel.evaluate \\\n",
        "    --base_model_path meta-llama/Meta-Llama-3-8B-Instruct \\\n",
        "    --model_path models/smoke_test_model/final_checkpoint \\\n",
        "    --test_set_path {smoke_val_path}\n",
        "\n",
        "print(\"\\n\\n✅ ✅ ✅ SMOKE TEST COMPLETE. If there were no errors, you are safe to proceed. ✅ ✅ ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CeyCBpnVK7R",
        "outputId": "8474bdaa-42aa-4697-c0a7-ae53a68d2110"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step A: Creating Smoke Test Data ---\n",
            "--- Step B: Running a 10-Step Smoke Test Training ---\n",
            "2025-08-12 03:13:33.513068: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-12 03:13:33.531428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754968413.553134    5588 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754968413.559806    5588 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754968413.577039    5588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754968413.577071    5588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754968413.577074    5588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754968413.577077    5588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-12 03:13:33.582213: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[1;34m--- Running Pre-flight Checks ---\u001b[0m\n",
            "Validating schema for training data: \n",
            "\u001b[36m/content/powershell-sentinel-main/data/generated/\u001b[0m\u001b[36msmoke_test_train.json\u001b[0m\u001b[95m...\u001b[0m\n",
            "\u001b[32mSchema validation passed for \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m training records.\u001b[0m\n",
            "Validating for data leakage from: \u001b[36mdata/sets/test_set_v0.json\u001b[0m\u001b[33m...\u001b[0m\n",
            "\u001b[32mData leakage check passed.\u001b[0m\n",
            "\u001b[1;32m--- Pre-flight Checks PASSED ---\u001b[0m\n",
            "\u001b[1;34m--- Starting training for model \u001b[0m\u001b[1;33mmeta-llama/Meta-Llama-\u001b[0m\u001b[1;33m3\u001b[0m\u001b[1;33m-8B-Instruct\u001b[0m\u001b[1;34m ---\u001b[0m\n",
            "Loaded and formatted \u001b[1;35m3\u001b[0m samples.\n",
            "Loading base model with \u001b[1;36m4\u001b[0m-bit quantization\u001b[33m...\u001b[0m\n",
            "Loading checkpoint shards: 100% 4/4 [00:10<00:00,  2.52s/it]\n",
            "tokenizer_config.json: 100% 51.0k/51.0k [00:00<00:00, 78.9MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 18.6MB/s]\n",
            "special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 523kB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Model and tokenizer loaded.\n",
            "Map: 100% 3/3 [00:00<00:00, 163.51 examples/s]\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "\u001b[1;34m--- Starting fine-tuning for \u001b[0m\u001b[1;33m10\u001b[0m\u001b[1;34m steps ---\u001b[0m\n",
            "{'loss': 0.8405, 'grad_norm': 0.9375, 'learning_rate': 2e-05, 'epoch': 6.67}\n",
            "{'train_runtime': 8.8416, 'train_samples_per_second': 4.524, 'train_steps_per_second': 1.131, 'train_loss': 0.8405202865600586, 'epoch': 6.67}\n",
            "100% 10/10 [00:08<00:00,  1.13it/s]\n",
            "\u001b[1;32m--- Fine-tuning complete ---\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "\u001b[1;32m--- Final adapter model saved to \u001b[0m\u001b[1;36mmodels/smoke_test_model/final_checkpoint\u001b[0m\u001b[1;32m ---\u001b[0m\n",
            "\n",
            "--- Step C: Running a 3-Sample Smoke Test Evaluation ---\n",
            "2025-08-12 03:14:08.744553: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-12 03:14:08.762313: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754968448.786157    5800 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754968448.792999    5800 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754968448.810031    5800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754968448.810067    5800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754968448.810071    5800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754968448.810076    5800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-12 03:14:08.815639: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loaded and validated \u001b[1;36m3\u001b[0m test samples.\n",
            "Loading base model and adapters\u001b[33m...\u001b[0m\n",
            "Loading checkpoint shards: 100% 4/4 [00:09<00:00,  2.31s/it]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Model loaded successfully.\n",
            "Running inference on test set\u001b[33m...\u001b[0m\n",
            "Evaluating:   0% 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  33% 1/3 [00:40<01:21, 40.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  67% 2/3 [01:23<00:41, 41.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating: 100% 3/3 [02:19<00:00, 46.65s/it]\n",
            "\u001b[3m PowerShell-Sentinel Final Evaluation \u001b[0m\n",
            "\u001b[3m                Report                \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m                    Metric\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mScore\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m             Total Samples\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m3    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m       Parse Success Count\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m       Parse Failure Count\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m3    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m   JSON Parse Success Rate\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.00%\u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m    Deobfuscation Accuracy\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.00%\u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m   Intent F1-Score (Macro)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.00%\u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36mMITRE TTP F1-Score (Macro)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.00%\u001b[0m\u001b[35m \u001b[0m│\n",
            "└────────────────────────────┴───────┘\n",
            "\n",
            "\n",
            "✅ ✅ ✅ SMOKE TEST COMPLETE. If there were no errors, you are safe to proceed. ✅ ✅ ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. DATASET CLEANING/PARTITIONING ---\n",
        "# --- Step 1: Clean Slate ---\n",
        "!rm -rf /content/powershell-sentinel-main/data/sets/*\n",
        "!rm -rf /content/powershell-sentinel-main/scripts/prompt_engineering/mini_*.json\n",
        "!rm -f /content/powershell-sentinel-main/data/generated/training_data_v0_clean.json\n",
        "\n",
        "# --- Step 2: Regenerate All Datasets Correctly ---\n",
        "!python -m scripts.deduplicate_dataset\n",
        "!python -m scripts.partition_dataset\n",
        "\n",
        "print(\"\\n\\n✅ DATASETS REGENERATED. LEAKAGE IS NOW IMPOSSIBLE. ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9VO9QQkZOjt",
        "outputId": "d2905a69-c0cf-4400-f79e-17fb13dfb1e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: Error while finding module specification for 'scripts.deduplicate_dataset' (ModuleNotFoundError: No module named 'scripts')\n",
            "/usr/bin/python3: Error while finding module specification for 'scripts.partition_dataset' (ModuleNotFoundError: No module named 'scripts')\n",
            "\n",
            "\n",
            "✅ DATASETS REGENERATED. LEAKAGE IS NOW IMPOSSIBLE. ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. FINAL MODEL TRAINING ---\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Use the NEWLY CREATED training_set_v0.json from the partition script\n",
        "final_train_path = '/content/powershell-sentinel-main/data/sets/training_set_v0.json'\n",
        "with open(final_train_path, 'r') as f:\n",
        "    num_samples = len(json.load(f))\n",
        "\n",
        "effective_batch_size = 4\n",
        "steps_per_epoch = num_samples // effective_batch_size\n",
        "max_steps_for_2_epochs = steps_per_epoch * 2\n",
        "\n",
        "print(f\"Final training set has {num_samples} samples.\")\n",
        "print(f\"Training for 2 epochs will take exactly {max_steps_for_2_epochs} steps.\")\n",
        "\n",
        "# Define paths for the command\n",
        "output_dir = \"models/final_model\"\n",
        "drive_backup_path = \"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/final_model_adapters.zip\"\n",
        "\n",
        "# --- THE CORRECTED COMMAND ---\n",
        "# This command is now simplified and matches the final train.py script.\n",
        "!python -m powershell_sentinel.train \\\n",
        "    --model_name meta-llama/Meta-Llama-3-8B-Instruct \\\n",
        "    --train_dataset {final_train_path} \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --test_dataset data/sets/test_set_v0.json \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --lora_rank 16 \\\n",
        "    --max_steps {max_steps_for_2_epochs}\n",
        "\n",
        "# --- Immediately back up the final model to Google Drive ---\n",
        "!mkdir -p /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable\n",
        "!zip -r {drive_backup_path} /content/powershell-sentinel-main/{output_dir}\n",
        "\n",
        "print(f\"\\n\\n✅ ✅ ✅ FINAL TRAINING COMPLETE AND BACKED UP TO DRIVE: {drive_backup_path} ✅ ✅ ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl-KpjI8ViTu",
        "outputId": "e30102f1-14a2-46cc-da58-b71540f08b2d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training set has 5117 samples.\n",
            "Training for 2 epochs will take exactly 2558 steps.\n",
            "2025-08-11 04:37:17.379820: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-11 04:37:17.398496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754887037.420206   13904 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754887037.426785   13904 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754887037.444030   13904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754887037.444068   13904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754887037.444071   13904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754887037.444075   13904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-11 04:37:17.448879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[1;34m--- Running Pre-flight Checks ---\u001b[0m\n",
            "Validating schema for training data: \n",
            "\u001b[36m/content/powershell-sentinel-main/data/sets/\u001b[0m\u001b[36mtraining_set_v0.json\u001b[0m\u001b[95m...\u001b[0m\n",
            "\u001b[32mSchema validation passed for \u001b[0m\u001b[1;32m5117\u001b[0m\u001b[32m training records.\u001b[0m\n",
            "Validating for data leakage from: \u001b[36mdata/sets/test_set_v0.json\u001b[0m\u001b[33m...\u001b[0m\n",
            "\u001b[32mData leakage check passed.\u001b[0m\n",
            "\u001b[1;32m--- Pre-flight Checks PASSED ---\u001b[0m\n",
            "\u001b[1;34m--- Starting training for model \u001b[0m\u001b[1;33mmeta-llama/Meta-Llama-\u001b[0m\u001b[1;33m3\u001b[0m\u001b[1;33m-8B-Instruct\u001b[0m\u001b[1;34m ---\u001b[0m\n",
            "Loaded and formatted \u001b[1;35m5117\u001b[0m samples.\n",
            "Loading base model with \u001b[1;36m4\u001b[0m-bit quantization\u001b[33m...\u001b[0m\n",
            "Loading checkpoint shards: 100% 4/4 [00:09<00:00,  2.31s/it]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Model and tokenizer loaded.\n",
            "Map: 100% 5117/5117 [00:01<00:00, 4110.86 examples/s]\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "\u001b[1;34m--- Starting fine-tuning for \u001b[0m\u001b[1;33m2558\u001b[0m\u001b[1;34m steps ---\u001b[0m\n",
            "{'loss': 1.6986, 'grad_norm': 1.3359375, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 1.4411, 'grad_norm': 1.3515625, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
            "{'loss': 1.0855, 'grad_norm': 1.578125, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
            "{'loss': 0.7387, 'grad_norm': 2.203125, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
            "{'loss': 0.5222, 'grad_norm': 1.53125, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
            "{'loss': 1.0158, 'grad_norm': 1.328125, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
            "{'loss': 0.7669, 'grad_norm': 1.328125, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
            "{'loss': 0.6207, 'grad_norm': 1.3984375, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
            "{'loss': 0.4625, 'grad_norm': 1.78125, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
            "{'loss': 0.3555, 'grad_norm': 1.703125, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
            "{'loss': 0.9004, 'grad_norm': 1.2421875, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
            "{'loss': 0.6062, 'grad_norm': 1.5, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5331, 'grad_norm': 1.4765625, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
            "{'loss': 0.4185, 'grad_norm': 1.5703125, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2848, 'grad_norm': 1.59375, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
            "{'loss': 0.8264, 'grad_norm': 1.5625, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
            "{'loss': 0.5289, 'grad_norm': 1.515625, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
            "{'loss': 0.47, 'grad_norm': 1.671875, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
            "{'loss': 0.345, 'grad_norm': 1.5546875, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
            "{'loss': 0.2478, 'grad_norm': 1.2421875, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
            "{'loss': 0.6727, 'grad_norm': 1.53125, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
            "{'loss': 0.4903, 'grad_norm': 1.8359375, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
            "{'loss': 0.4327, 'grad_norm': 1.3515625, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
            "{'loss': 0.3053, 'grad_norm': 1.5703125, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2211, 'grad_norm': 1.375, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
            "{'loss': 0.6378, 'grad_norm': 1.828125, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
            "{'loss': 0.3793, 'grad_norm': 1.8828125, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
            "{'loss': 0.3902, 'grad_norm': 2.0625, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
            "{'loss': 0.3029, 'grad_norm': 1.5625, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
            "{'loss': 0.1949, 'grad_norm': 1.9765625, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
            "{'loss': 0.5643, 'grad_norm': 1.203125, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
            "{'loss': 0.4133, 'grad_norm': 1.9453125, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
            "{'loss': 0.3672, 'grad_norm': 1.5625, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
            "{'loss': 0.2802, 'grad_norm': 1.640625, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
            "{'loss': 0.1966, 'grad_norm': 1.578125, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
            "{'loss': 0.5753, 'grad_norm': 1.65625, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
            "{'loss': 0.332, 'grad_norm': 2.421875, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
            "{'loss': 0.3537, 'grad_norm': 1.953125, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
            "{'loss': 0.2454, 'grad_norm': 1.6171875, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
            "{'loss': 0.1866, 'grad_norm': 1.6796875, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
            "{'loss': 0.5831, 'grad_norm': 1.3984375, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
            "{'loss': 0.3533, 'grad_norm': 1.4921875, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
            "{'loss': 0.3085, 'grad_norm': 2.046875, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
            "{'loss': 0.2332, 'grad_norm': 1.5625, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
            "{'loss': 0.1788, 'grad_norm': 1.5390625, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
            "{'loss': 0.5517, 'grad_norm': 1.6015625, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
            "{'loss': 0.354, 'grad_norm': 1.5546875, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
            "{'loss': 0.309, 'grad_norm': 1.5546875, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2407, 'grad_norm': 1.3984375, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
            "{'loss': 0.1753, 'grad_norm': 1.8828125, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
            " 20% 500/2558 [13:46<55:23,  1.61s/it]/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "{'loss': 0.502, 'grad_norm': 1.015625, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
            "{'loss': 0.3292, 'grad_norm': 1.875, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
            "{'loss': 0.2907, 'grad_norm': 1.546875, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
            "{'loss': 0.216, 'grad_norm': 1.40625, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
            "{'loss': 0.1528, 'grad_norm': 1.4921875, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
            "{'loss': 0.4836, 'grad_norm': 1.296875, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
            "{'loss': 0.2762, 'grad_norm': 1.921875, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
            "{'loss': 0.3003, 'grad_norm': 1.7421875, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2472, 'grad_norm': 1.7734375, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
            "{'loss': 0.1523, 'grad_norm': 2.234375, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4435, 'grad_norm': 1.1796875, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2719, 'grad_norm': 1.609375, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2711, 'grad_norm': 1.8359375, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
            "{'loss': 0.2144, 'grad_norm': 1.4453125, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
            "{'loss': 0.1555, 'grad_norm': 1.890625, 'learning_rate': 2e-05, 'epoch': 0.51}\n",
            "{'loss': 0.4228, 'grad_norm': 1.390625, 'learning_rate': 2e-05, 'epoch': 0.52}\n",
            "{'loss': 0.2823, 'grad_norm': 2.03125, 'learning_rate': 2e-05, 'epoch': 0.52}\n",
            "{'loss': 0.2491, 'grad_norm': 1.4765625, 'learning_rate': 2e-05, 'epoch': 0.53}\n",
            "{'loss': 0.202, 'grad_norm': 1.0078125, 'learning_rate': 2e-05, 'epoch': 0.54}\n",
            "{'loss': 0.1453, 'grad_norm': 2.4375, 'learning_rate': 2e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4393, 'grad_norm': 0.99609375, 'learning_rate': 2e-05, 'epoch': 0.56}\n",
            "{'loss': 0.2554, 'grad_norm': 1.3828125, 'learning_rate': 2e-05, 'epoch': 0.56}\n",
            "{'loss': 0.2422, 'grad_norm': 2.03125, 'learning_rate': 2e-05, 'epoch': 0.57}\n",
            "{'loss': 0.1937, 'grad_norm': 1.40625, 'learning_rate': 2e-05, 'epoch': 0.58}\n",
            "{'loss': 0.1355, 'grad_norm': 1.5, 'learning_rate': 2e-05, 'epoch': 0.59}\n",
            "{'loss': 0.3936, 'grad_norm': 1.109375, 'learning_rate': 2e-05, 'epoch': 0.59}\n",
            "{'loss': 0.2372, 'grad_norm': 1.6328125, 'learning_rate': 2e-05, 'epoch': 0.6}\n",
            "{'loss': 0.2416, 'grad_norm': 1.7421875, 'learning_rate': 2e-05, 'epoch': 0.61}\n",
            "{'loss': 0.1927, 'grad_norm': 1.3984375, 'learning_rate': 2e-05, 'epoch': 0.62}\n",
            "{'loss': 0.1481, 'grad_norm': 2.75, 'learning_rate': 2e-05, 'epoch': 0.63}\n",
            "{'loss': 0.3937, 'grad_norm': 0.9375, 'learning_rate': 2e-05, 'epoch': 0.63}\n",
            "{'loss': 0.2513, 'grad_norm': 1.875, 'learning_rate': 2e-05, 'epoch': 0.64}\n",
            "{'loss': 0.2637, 'grad_norm': 1.8125, 'learning_rate': 2e-05, 'epoch': 0.65}\n",
            "{'loss': 0.183, 'grad_norm': 1.359375, 'learning_rate': 2e-05, 'epoch': 0.66}\n",
            "{'loss': 0.1549, 'grad_norm': 1.7734375, 'learning_rate': 2e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4417, 'grad_norm': 2.109375, 'learning_rate': 2e-05, 'epoch': 0.67}\n",
            "{'loss': 0.2225, 'grad_norm': 1.6875, 'learning_rate': 2e-05, 'epoch': 0.68}\n",
            "{'loss': 0.2396, 'grad_norm': 1.8828125, 'learning_rate': 2e-05, 'epoch': 0.69}\n",
            "{'loss': 0.1772, 'grad_norm': 1.328125, 'learning_rate': 2e-05, 'epoch': 0.7}\n",
            "{'loss': 0.1307, 'grad_norm': 1.5, 'learning_rate': 2e-05, 'epoch': 0.7}\n",
            "{'loss': 0.3871, 'grad_norm': 1.7578125, 'learning_rate': 2e-05, 'epoch': 0.71}\n",
            "{'loss': 0.2351, 'grad_norm': 1.53125, 'learning_rate': 2e-05, 'epoch': 0.72}\n",
            "{'loss': 0.2205, 'grad_norm': 1.9453125, 'learning_rate': 2e-05, 'epoch': 0.73}\n",
            "{'loss': 0.1672, 'grad_norm': 1.3984375, 'learning_rate': 2e-05, 'epoch': 0.73}\n",
            "{'loss': 0.1222, 'grad_norm': 1.3515625, 'learning_rate': 2e-05, 'epoch': 0.74}\n",
            "{'loss': 0.3772, 'grad_norm': 1.7421875, 'learning_rate': 2e-05, 'epoch': 0.75}\n",
            "{'loss': 0.2077, 'grad_norm': 1.59375, 'learning_rate': 2e-05, 'epoch': 0.76}\n",
            "{'loss': 0.2062, 'grad_norm': 1.7109375, 'learning_rate': 2e-05, 'epoch': 0.77}\n",
            "{'loss': 0.1617, 'grad_norm': 1.828125, 'learning_rate': 2e-05, 'epoch': 0.77}\n",
            "{'loss': 0.111, 'grad_norm': 2.15625, 'learning_rate': 2e-05, 'epoch': 0.78}\n",
            " 39% 1000/2558 [27:34<41:43,  1.61s/it]/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "{'loss': 0.3895, 'grad_norm': 2.109375, 'learning_rate': 2e-05, 'epoch': 0.79}\n",
            "{'loss': 0.2208, 'grad_norm': 1.3515625, 'learning_rate': 2e-05, 'epoch': 0.8}\n",
            "{'loss': 0.2227, 'grad_norm': 1.6796875, 'learning_rate': 2e-05, 'epoch': 0.81}\n",
            "{'loss': 0.1558, 'grad_norm': 1.0, 'learning_rate': 2e-05, 'epoch': 0.81}\n",
            "{'loss': 0.1329, 'grad_norm': 2.140625, 'learning_rate': 2e-05, 'epoch': 0.82}\n",
            "{'loss': 0.3799, 'grad_norm': 1.921875, 'learning_rate': 2e-05, 'epoch': 0.83}\n",
            "{'loss': 0.2048, 'grad_norm': 1.515625, 'learning_rate': 2e-05, 'epoch': 0.84}\n",
            "{'loss': 0.2157, 'grad_norm': 1.453125, 'learning_rate': 2e-05, 'epoch': 0.84}\n",
            "{'loss': 0.1461, 'grad_norm': 1.84375, 'learning_rate': 2e-05, 'epoch': 0.85}\n",
            "{'loss': 0.1156, 'grad_norm': 1.09375, 'learning_rate': 2e-05, 'epoch': 0.86}\n",
            "{'loss': 0.3374, 'grad_norm': 1.9453125, 'learning_rate': 2e-05, 'epoch': 0.87}\n",
            "{'loss': 0.202, 'grad_norm': 1.78125, 'learning_rate': 2e-05, 'epoch': 0.88}\n",
            "{'loss': 0.2034, 'grad_norm': 1.34375, 'learning_rate': 2e-05, 'epoch': 0.88}\n",
            "{'loss': 0.161, 'grad_norm': 1.1640625, 'learning_rate': 2e-05, 'epoch': 0.89}\n",
            "{'loss': 0.1016, 'grad_norm': 1.5234375, 'learning_rate': 2e-05, 'epoch': 0.9}\n",
            "{'loss': 0.3307, 'grad_norm': 1.9921875, 'learning_rate': 2e-05, 'epoch': 0.91}\n",
            "{'loss': 0.2175, 'grad_norm': 1.140625, 'learning_rate': 2e-05, 'epoch': 0.91}\n",
            "{'loss': 0.1887, 'grad_norm': 1.46875, 'learning_rate': 2e-05, 'epoch': 0.92}\n",
            "{'loss': 0.1493, 'grad_norm': 1.6875, 'learning_rate': 2e-05, 'epoch': 0.93}\n",
            "{'loss': 0.107, 'grad_norm': 1.78125, 'learning_rate': 2e-05, 'epoch': 0.94}\n",
            "{'loss': 0.3202, 'grad_norm': 1.4296875, 'learning_rate': 2e-05, 'epoch': 0.95}\n",
            "{'loss': 0.1902, 'grad_norm': 1.3203125, 'learning_rate': 2e-05, 'epoch': 0.95}\n",
            "{'loss': 0.1782, 'grad_norm': 1.1640625, 'learning_rate': 2e-05, 'epoch': 0.96}\n",
            "{'loss': 0.1446, 'grad_norm': 1.2265625, 'learning_rate': 2e-05, 'epoch': 0.97}\n",
            "{'loss': 0.1025, 'grad_norm': 1.3984375, 'learning_rate': 2e-05, 'epoch': 0.98}\n",
            "{'loss': 0.256, 'grad_norm': 1.46875, 'learning_rate': 2e-05, 'epoch': 0.98}\n",
            "{'loss': 0.2012, 'grad_norm': 1.3671875, 'learning_rate': 2e-05, 'epoch': 0.99}\n",
            "{'loss': 0.1442, 'grad_norm': 1.6015625, 'learning_rate': 2e-05, 'epoch': 1.0}\n",
            "{'loss': 0.2847, 'grad_norm': 1.53125, 'learning_rate': 2e-05, 'epoch': 1.01}\n",
            "{'loss': 0.1914, 'grad_norm': 1.1640625, 'learning_rate': 2e-05, 'epoch': 1.02}\n",
            "{'loss': 0.1703, 'grad_norm': 1.671875, 'learning_rate': 2e-05, 'epoch': 1.02}\n",
            "{'loss': 0.1238, 'grad_norm': 1.1015625, 'learning_rate': 2e-05, 'epoch': 1.03}\n",
            "{'loss': 0.1109, 'grad_norm': 1.6796875, 'learning_rate': 2e-05, 'epoch': 1.04}\n",
            "{'loss': 0.3132, 'grad_norm': 1.09375, 'learning_rate': 2e-05, 'epoch': 1.05}\n",
            "{'loss': 0.1568, 'grad_norm': 0.97265625, 'learning_rate': 2e-05, 'epoch': 1.06}\n",
            "{'loss': 0.1812, 'grad_norm': 1.78125, 'learning_rate': 2e-05, 'epoch': 1.06}\n",
            "{'loss': 0.1363, 'grad_norm': 1.4140625, 'learning_rate': 2e-05, 'epoch': 1.07}\n",
            "{'loss': 0.114, 'grad_norm': 1.84375, 'learning_rate': 2e-05, 'epoch': 1.08}\n",
            "{'loss': 0.2841, 'grad_norm': 1.6015625, 'learning_rate': 2e-05, 'epoch': 1.09}\n",
            "{'loss': 0.1781, 'grad_norm': 1.28125, 'learning_rate': 2e-05, 'epoch': 1.09}\n",
            "{'loss': 0.1715, 'grad_norm': 1.6171875, 'learning_rate': 2e-05, 'epoch': 1.1}\n",
            "{'loss': 0.1187, 'grad_norm': 0.921875, 'learning_rate': 2e-05, 'epoch': 1.11}\n",
            "{'loss': 0.1153, 'grad_norm': 1.9765625, 'learning_rate': 2e-05, 'epoch': 1.12}\n",
            "{'loss': 0.2893, 'grad_norm': 1.421875, 'learning_rate': 2e-05, 'epoch': 1.13}\n",
            "{'loss': 0.1718, 'grad_norm': 1.328125, 'learning_rate': 2e-05, 'epoch': 1.13}\n",
            "{'loss': 0.1554, 'grad_norm': 1.4296875, 'learning_rate': 2e-05, 'epoch': 1.14}\n",
            "{'loss': 0.1305, 'grad_norm': 1.4609375, 'learning_rate': 2e-05, 'epoch': 1.15}\n",
            "{'loss': 0.1099, 'grad_norm': 2.71875, 'learning_rate': 2e-05, 'epoch': 1.16}\n",
            "{'loss': 0.2675, 'grad_norm': 0.8671875, 'learning_rate': 2e-05, 'epoch': 1.16}\n",
            "{'loss': 0.153, 'grad_norm': 1.1796875, 'learning_rate': 2e-05, 'epoch': 1.17}\n",
            " 59% 1500/2558 [41:26<28:47,  1.63s/it]/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "{'loss': 0.1756, 'grad_norm': 1.1796875, 'learning_rate': 2e-05, 'epoch': 1.18}\n",
            "{'loss': 0.15, 'grad_norm': 1.2109375, 'learning_rate': 2e-05, 'epoch': 1.19}\n",
            "{'loss': 0.1062, 'grad_norm': 1.46875, 'learning_rate': 2e-05, 'epoch': 1.2}\n",
            "{'loss': 0.2626, 'grad_norm': 1.359375, 'learning_rate': 2e-05, 'epoch': 1.2}\n",
            "{'loss': 0.1615, 'grad_norm': 1.0390625, 'learning_rate': 2e-05, 'epoch': 1.21}\n",
            "{'loss': 0.1546, 'grad_norm': 1.28125, 'learning_rate': 2e-05, 'epoch': 1.22}\n",
            "{'loss': 0.1321, 'grad_norm': 2.625, 'learning_rate': 2e-05, 'epoch': 1.23}\n",
            "{'loss': 0.1099, 'grad_norm': 1.4609375, 'learning_rate': 2e-05, 'epoch': 1.24}\n",
            "{'loss': 0.2301, 'grad_norm': 1.2265625, 'learning_rate': 2e-05, 'epoch': 1.24}\n",
            "{'loss': 0.1438, 'grad_norm': 1.5390625, 'learning_rate': 2e-05, 'epoch': 1.25}\n",
            "{'loss': 0.1676, 'grad_norm': 1.296875, 'learning_rate': 2e-05, 'epoch': 1.26}\n",
            "{'loss': 0.1235, 'grad_norm': 0.90625, 'learning_rate': 2e-05, 'epoch': 1.27}\n",
            "{'loss': 0.1164, 'grad_norm': 1.328125, 'learning_rate': 2e-05, 'epoch': 1.27}\n",
            "{'loss': 0.2609, 'grad_norm': 1.515625, 'learning_rate': 2e-05, 'epoch': 1.28}\n",
            "{'loss': 0.1517, 'grad_norm': 1.2265625, 'learning_rate': 2e-05, 'epoch': 1.29}\n",
            "{'loss': 0.1723, 'grad_norm': 1.34375, 'learning_rate': 2e-05, 'epoch': 1.3}\n",
            "{'loss': 0.1296, 'grad_norm': 1.3671875, 'learning_rate': 2e-05, 'epoch': 1.31}\n",
            "{'loss': 0.109, 'grad_norm': 1.3203125, 'learning_rate': 2e-05, 'epoch': 1.31}\n",
            "{'loss': 0.2544, 'grad_norm': 1.34375, 'learning_rate': 2e-05, 'epoch': 1.32}\n",
            "{'loss': 0.1703, 'grad_norm': 1.4765625, 'learning_rate': 2e-05, 'epoch': 1.33}\n",
            "{'loss': 0.1439, 'grad_norm': 1.390625, 'learning_rate': 2e-05, 'epoch': 1.34}\n",
            "{'loss': 0.1135, 'grad_norm': 1.0859375, 'learning_rate': 2e-05, 'epoch': 1.34}\n",
            "{'loss': 0.1003, 'grad_norm': 1.390625, 'learning_rate': 2e-05, 'epoch': 1.35}\n",
            "{'loss': 0.2608, 'grad_norm': 1.390625, 'learning_rate': 2e-05, 'epoch': 1.36}\n",
            "{'loss': 0.176, 'grad_norm': 1.09375, 'learning_rate': 2e-05, 'epoch': 1.37}\n",
            "{'loss': 0.1524, 'grad_norm': 1.203125, 'learning_rate': 2e-05, 'epoch': 1.38}\n",
            "{'loss': 0.116, 'grad_norm': 1.125, 'learning_rate': 2e-05, 'epoch': 1.38}\n",
            "{'loss': 0.1023, 'grad_norm': 1.1796875, 'learning_rate': 2e-05, 'epoch': 1.39}\n",
            "{'loss': 0.2382, 'grad_norm': 1.6953125, 'learning_rate': 2e-05, 'epoch': 1.4}\n",
            "{'loss': 0.1531, 'grad_norm': 0.95703125, 'learning_rate': 2e-05, 'epoch': 1.41}\n",
            "{'loss': 0.1472, 'grad_norm': 1.2578125, 'learning_rate': 2e-05, 'epoch': 1.41}\n",
            "{'loss': 0.1299, 'grad_norm': 2.4375, 'learning_rate': 2e-05, 'epoch': 1.42}\n",
            "{'loss': 0.1157, 'grad_norm': 2.140625, 'learning_rate': 2e-05, 'epoch': 1.43}\n",
            "{'loss': 0.2235, 'grad_norm': 1.4140625, 'learning_rate': 2e-05, 'epoch': 1.44}\n",
            "{'loss': 0.1477, 'grad_norm': 1.0859375, 'learning_rate': 2e-05, 'epoch': 1.45}\n",
            "{'loss': 0.1611, 'grad_norm': 1.15625, 'learning_rate': 2e-05, 'epoch': 1.45}\n",
            "{'loss': 0.1154, 'grad_norm': 0.8828125, 'learning_rate': 2e-05, 'epoch': 1.46}\n",
            "{'loss': 0.1128, 'grad_norm': 1.3359375, 'learning_rate': 2e-05, 'epoch': 1.47}\n",
            "{'loss': 0.2374, 'grad_norm': 1.578125, 'learning_rate': 2e-05, 'epoch': 1.48}\n",
            "{'loss': 0.1673, 'grad_norm': 1.4375, 'learning_rate': 2e-05, 'epoch': 1.49}\n",
            "{'loss': 0.1264, 'grad_norm': 1.2734375, 'learning_rate': 2e-05, 'epoch': 1.49}\n",
            "{'loss': 0.1146, 'grad_norm': 0.8515625, 'learning_rate': 2e-05, 'epoch': 1.5}\n",
            "{'loss': 0.1031, 'grad_norm': 1.8359375, 'learning_rate': 2e-05, 'epoch': 1.51}\n",
            "{'loss': 0.2714, 'grad_norm': 1.6796875, 'learning_rate': 2e-05, 'epoch': 1.52}\n",
            "{'loss': 0.1576, 'grad_norm': 1.203125, 'learning_rate': 2e-05, 'epoch': 1.52}\n",
            "{'loss': 0.1466, 'grad_norm': 1.3828125, 'learning_rate': 2e-05, 'epoch': 1.53}\n",
            "{'loss': 0.1223, 'grad_norm': 0.82421875, 'learning_rate': 2e-05, 'epoch': 1.54}\n",
            "{'loss': 0.1101, 'grad_norm': 1.296875, 'learning_rate': 2e-05, 'epoch': 1.55}\n",
            "{'loss': 0.2687, 'grad_norm': 1.515625, 'learning_rate': 2e-05, 'epoch': 1.56}\n",
            "{'loss': 0.1472, 'grad_norm': 0.9140625, 'learning_rate': 2e-05, 'epoch': 1.56}\n",
            " 78% 2000/2558 [55:15<15:02,  1.62s/it]/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "{'loss': 0.1434, 'grad_norm': 1.453125, 'learning_rate': 2e-05, 'epoch': 1.57}\n",
            "{'loss': 0.1193, 'grad_norm': 1.015625, 'learning_rate': 2e-05, 'epoch': 1.58}\n",
            "{'loss': 0.0986, 'grad_norm': 1.3203125, 'learning_rate': 2e-05, 'epoch': 1.59}\n",
            "{'loss': 0.2603, 'grad_norm': 1.609375, 'learning_rate': 2e-05, 'epoch': 1.59}\n",
            "{'loss': 0.1543, 'grad_norm': 0.70703125, 'learning_rate': 2e-05, 'epoch': 1.6}\n",
            "{'loss': 0.1421, 'grad_norm': 1.4375, 'learning_rate': 2e-05, 'epoch': 1.61}\n",
            "{'loss': 0.1165, 'grad_norm': 0.828125, 'learning_rate': 2e-05, 'epoch': 1.62}\n",
            "{'loss': 0.1001, 'grad_norm': 1.5, 'learning_rate': 2e-05, 'epoch': 1.63}\n",
            "{'loss': 0.2404, 'grad_norm': 1.453125, 'learning_rate': 2e-05, 'epoch': 1.63}\n",
            "{'loss': 0.1232, 'grad_norm': 1.109375, 'learning_rate': 2e-05, 'epoch': 1.64}\n",
            "{'loss': 0.1554, 'grad_norm': 1.109375, 'learning_rate': 2e-05, 'epoch': 1.65}\n",
            "{'loss': 0.115, 'grad_norm': 0.8203125, 'learning_rate': 2e-05, 'epoch': 1.66}\n",
            "{'loss': 0.1019, 'grad_norm': 1.1484375, 'learning_rate': 2e-05, 'epoch': 1.67}\n",
            "{'loss': 0.2424, 'grad_norm': 1.234375, 'learning_rate': 2e-05, 'epoch': 1.67}\n",
            "{'loss': 0.1449, 'grad_norm': 1.109375, 'learning_rate': 2e-05, 'epoch': 1.68}\n",
            "{'loss': 0.1702, 'grad_norm': 1.078125, 'learning_rate': 2e-05, 'epoch': 1.69}\n",
            "{'loss': 0.1172, 'grad_norm': 0.80859375, 'learning_rate': 2e-05, 'epoch': 1.7}\n",
            "{'loss': 0.1076, 'grad_norm': 1.1015625, 'learning_rate': 2e-05, 'epoch': 1.7}\n",
            "{'loss': 0.2154, 'grad_norm': 1.4140625, 'learning_rate': 2e-05, 'epoch': 1.71}\n",
            "{'loss': 0.1601, 'grad_norm': 0.66015625, 'learning_rate': 2e-05, 'epoch': 1.72}\n",
            "{'loss': 0.1411, 'grad_norm': 1.1015625, 'learning_rate': 2e-05, 'epoch': 1.73}\n",
            "{'loss': 0.117, 'grad_norm': 2.546875, 'learning_rate': 2e-05, 'epoch': 1.74}\n",
            "{'loss': 0.0996, 'grad_norm': 1.3671875, 'learning_rate': 2e-05, 'epoch': 1.74}\n",
            "{'loss': 0.2004, 'grad_norm': 0.6484375, 'learning_rate': 2e-05, 'epoch': 1.75}\n",
            "{'loss': 0.1419, 'grad_norm': 1.359375, 'learning_rate': 2e-05, 'epoch': 1.76}\n",
            "{'loss': 0.1385, 'grad_norm': 1.1484375, 'learning_rate': 2e-05, 'epoch': 1.77}\n",
            "{'loss': 0.1064, 'grad_norm': 1.8046875, 'learning_rate': 2e-05, 'epoch': 1.77}\n",
            "{'loss': 0.1047, 'grad_norm': 1.78125, 'learning_rate': 2e-05, 'epoch': 1.78}\n",
            "{'loss': 0.2211, 'grad_norm': 1.3046875, 'learning_rate': 2e-05, 'epoch': 1.79}\n",
            "{'loss': 0.1543, 'grad_norm': 0.984375, 'learning_rate': 2e-05, 'epoch': 1.8}\n",
            "{'loss': 0.1348, 'grad_norm': 1.1484375, 'learning_rate': 2e-05, 'epoch': 1.81}\n",
            "{'loss': 0.1069, 'grad_norm': 0.9453125, 'learning_rate': 2e-05, 'epoch': 1.81}\n",
            "{'loss': 0.1014, 'grad_norm': 1.203125, 'learning_rate': 2e-05, 'epoch': 1.82}\n",
            "{'loss': 0.219, 'grad_norm': 1.71875, 'learning_rate': 2e-05, 'epoch': 1.83}\n",
            "{'loss': 0.1304, 'grad_norm': 1.0703125, 'learning_rate': 2e-05, 'epoch': 1.84}\n",
            "{'loss': 0.1411, 'grad_norm': 1.3203125, 'learning_rate': 2e-05, 'epoch': 1.84}\n",
            "{'loss': 0.1107, 'grad_norm': 1.0859375, 'learning_rate': 2e-05, 'epoch': 1.85}\n",
            "{'loss': 0.103, 'grad_norm': 1.25, 'learning_rate': 2e-05, 'epoch': 1.86}\n",
            "{'loss': 0.2353, 'grad_norm': 0.66015625, 'learning_rate': 2e-05, 'epoch': 1.87}\n",
            "{'loss': 0.1424, 'grad_norm': 1.140625, 'learning_rate': 2e-05, 'epoch': 1.88}\n",
            "{'loss': 0.1294, 'grad_norm': 1.2734375, 'learning_rate': 2e-05, 'epoch': 1.88}\n",
            "{'loss': 0.1165, 'grad_norm': 1.265625, 'learning_rate': 2e-05, 'epoch': 1.89}\n",
            "{'loss': 0.1074, 'grad_norm': 1.484375, 'learning_rate': 2e-05, 'epoch': 1.9}\n",
            "{'loss': 0.2262, 'grad_norm': 1.140625, 'learning_rate': 2e-05, 'epoch': 1.91}\n",
            "{'loss': 0.1456, 'grad_norm': 0.953125, 'learning_rate': 2e-05, 'epoch': 1.92}\n",
            "{'loss': 0.1315, 'grad_norm': 1.1171875, 'learning_rate': 2e-05, 'epoch': 1.92}\n",
            "{'loss': 0.1175, 'grad_norm': 0.5859375, 'learning_rate': 2e-05, 'epoch': 1.93}\n",
            "{'loss': 0.1028, 'grad_norm': 1.4765625, 'learning_rate': 2e-05, 'epoch': 1.94}\n",
            "{'loss': 0.226, 'grad_norm': 0.91015625, 'learning_rate': 2e-05, 'epoch': 1.95}\n",
            "{'loss': 0.1289, 'grad_norm': 1.25, 'learning_rate': 2e-05, 'epoch': 1.95}\n",
            " 98% 2500/2558 [1:09:05<01:33,  1.62s/it]/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "{'loss': 0.1504, 'grad_norm': 0.9453125, 'learning_rate': 2e-05, 'epoch': 1.96}\n",
            "{'loss': 0.1118, 'grad_norm': 0.640625, 'learning_rate': 2e-05, 'epoch': 1.97}\n",
            "{'loss': 0.0997, 'grad_norm': 1.640625, 'learning_rate': 2e-05, 'epoch': 1.98}\n",
            "{'loss': 0.2038, 'grad_norm': 1.125, 'learning_rate': 2e-05, 'epoch': 1.99}\n",
            "{'loss': 0.1426, 'grad_norm': 0.95703125, 'learning_rate': 2e-05, 'epoch': 1.99}\n",
            "{'train_runtime': 4241.5835, 'train_samples_per_second': 2.412, 'train_steps_per_second': 0.603, 'train_loss': 0.24687357077941566, 'epoch': 2.0}\n",
            "100% 2558/2558 [1:10:41<00:00,  1.66s/it]\n",
            "\u001b[1;32m--- Fine-tuning complete ---\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "\u001b[1;32m--- Final adapter model saved to \u001b[0m\u001b[1;36mmodels/final_model/final_checkpoint\u001b[0m\u001b[1;32m ---\u001b[0m\n",
            "updating: content/powershell-sentinel-main/models/final_model/ (stored 0%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/ (stored 0%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/trainer_state.json (deflated 84%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/adapter_config.json (deflated 54%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/tokenizer.json (deflated 74%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/README.md (deflated 66%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/training_args.bin (deflated 51%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/tokenizer_config.json (deflated 96%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/optimizer.pt (deflated 16%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/adapter_model.safetensors (deflated 22%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/special_tokens_map.json (deflated 63%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/scheduler.pt (deflated 57%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1000/rng_state.pth (deflated 25%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/final_checkpoint/ (stored 0%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/final_checkpoint/adapter_config.json (deflated 54%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/final_checkpoint/tokenizer.json (deflated 74%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/final_checkpoint/README.md (deflated 66%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/final_checkpoint/training_args.bin (deflated 51%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/final_checkpoint/tokenizer_config.json (deflated 96%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/final_checkpoint/adapter_model.safetensors (deflated 21%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/final_checkpoint/special_tokens_map.json (deflated 63%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/ (stored 0%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/trainer_state.json (deflated 85%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/adapter_config.json (deflated 54%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/tokenizer.json (deflated 74%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/README.md (deflated 66%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/training_args.bin (deflated 51%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/tokenizer_config.json (deflated 96%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/optimizer.pt (deflated 16%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/adapter_model.safetensors (deflated 22%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/special_tokens_map.json (deflated 63%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/scheduler.pt (deflated 57%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-1500/rng_state.pth (deflated 25%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/ (stored 0%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/trainer_state.json (deflated 85%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/adapter_config.json (deflated 54%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/tokenizer.json (deflated 74%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/README.md (deflated 66%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/training_args.bin (deflated 51%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/tokenizer_config.json (deflated 96%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/optimizer.pt (deflated 16%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/adapter_model.safetensors (deflated 21%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/special_tokens_map.json (deflated 63%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/scheduler.pt (deflated 57%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2000/rng_state.pth (deflated 25%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/ (stored 0%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/trainer_state.json (deflated 81%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/adapter_config.json (deflated 54%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/tokenizer.json (deflated 74%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/README.md (deflated 66%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/training_args.bin (deflated 51%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/tokenizer_config.json (deflated 96%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/optimizer.pt (deflated 16%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/adapter_model.safetensors (deflated 22%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/special_tokens_map.json (deflated 63%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/scheduler.pt (deflated 57%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-500/rng_state.pth (deflated 25%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/ (stored 0%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/trainer_state.json (deflated 86%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/adapter_config.json (deflated 54%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/tokenizer.json (deflated 74%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/README.md (deflated 66%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/training_args.bin (deflated 51%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/tokenizer_config.json (deflated 96%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/optimizer.pt (deflated 16%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/adapter_model.safetensors (deflated 21%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/special_tokens_map.json (deflated 63%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/scheduler.pt (deflated 57%)\n",
            "  adding: content/powershell-sentinel-main/models/final_model/checkpoint-2500/rng_state.pth (deflated 25%)\n",
            "\n",
            "\n",
            "✅ ✅ ✅ FINAL TRAINING COMPLETE AND BACKED UP TO DRIVE: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/final_model_adapters.zip ✅ ✅ ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. FINAL MODEL EVALUATION ---\n",
        "# This will produce the final results table for your dissertation.\n",
        "report_filename = \"final_evaluation_report.md\"\n",
        "local_report_path = f\"/content/powershell-sentinel-main/{report_filename}\"\n",
        "drive_report_path = f\"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/{report_filename}\"\n",
        "\n",
        "!python -m powershell_sentinel.evaluate \\\n",
        "    --base_model_path meta-llama/Meta-Llama-3-8B-Instruct \\\n",
        "    --model_path models/final_model/final_checkpoint \\\n",
        "    --test_set_path data/sets/test_set_v0.json > {local_report_path}\n",
        "\n",
        "# --- Immediately back up the final report to Google Drive ---\n",
        "!cp {local_report_path} {drive_report_path}\n",
        "\n",
        "# Print the report to the screen as well\n",
        "!cat {local_report_path}\n",
        "\n",
        "print(f\"\\n\\n✅ ✅ ✅ FINAL EVALUATION COMPLETE AND REPORT BACKED UP TO DRIVE. ✅ ✅ ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d3NApB2Vmwu",
        "outputId": "aaf1db71-07d9-445c-bf34-bd3cf40f359c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-11 05:49:57.443821: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-11 05:49:57.462662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754891397.484259   32541 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754891397.490836   32541 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754891397.508490   32541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754891397.508531   32541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754891397.508534   32541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754891397.508537   32541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-11 05:49:57.513519: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading checkpoint shards: 100% 4/4 [00:10<00:00,  2.63s/it]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Evaluating:   0% 0/569 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   0% 1/569 [00:16<2:32:18, 16.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   0% 2/569 [00:38<3:06:25, 19.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   1% 3/569 [00:58<3:06:20, 19.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   1% 4/569 [01:17<3:05:37, 19.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   1% 5/569 [01:37<3:06:49, 19.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   1% 6/569 [01:58<3:07:15, 19.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   1% 7/569 [02:41<4:17:36, 27.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   1% 8/569 [02:56<3:39:55, 23.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   2% 9/569 [03:15<3:28:02, 22.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   2% 10/569 [03:33<3:15:08, 20.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   2% 11/569 [03:53<3:11:01, 20.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   2% 12/569 [04:11<3:03:10, 19.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   2% 13/569 [04:27<2:52:23, 18.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   2% 14/569 [04:51<3:06:54, 20.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   3% 15/569 [05:12<3:11:10, 20.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   3% 16/569 [05:30<3:02:49, 19.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   3% 17/569 [05:46<2:51:26, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   3% 18/569 [06:29<3:57:51, 25.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   3% 19/569 [06:45<3:29:51, 22.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   4% 20/569 [07:04<3:20:08, 21.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   4% 21/569 [07:20<3:02:59, 20.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   4% 22/569 [07:37<2:52:59, 18.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   4% 23/569 [07:52<2:41:52, 17.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   4% 24/569 [08:07<2:36:04, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   4% 25/569 [08:27<2:42:32, 17.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   5% 26/569 [08:45<2:42:02, 17.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   5% 27/569 [09:04<2:45:50, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   5% 28/569 [09:20<2:38:45, 17.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   5% 29/569 [09:36<2:32:38, 16.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   5% 30/569 [09:53<2:33:35, 17.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   5% 31/569 [10:11<2:34:59, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   6% 32/569 [10:34<2:50:52, 19.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   6% 33/569 [10:50<2:40:57, 18.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   6% 34/569 [11:05<2:33:59, 17.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   6% 35/569 [11:21<2:31:27, 17.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   6% 36/569 [11:45<2:48:13, 18.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   7% 37/569 [12:01<2:41:10, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   7% 38/569 [12:22<2:46:28, 18.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   7% 39/569 [13:31<4:59:44, 33.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   7% 40/569 [13:47<4:11:36, 28.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   7% 41/569 [14:05<3:42:46, 25.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   7% 42/569 [14:22<3:22:23, 23.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   8% 43/569 [14:46<3:24:27, 23.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   8% 44/569 [15:03<3:06:58, 21.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   8% 45/569 [16:12<5:11:16, 35.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   8% 46/569 [16:32<4:28:52, 30.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   8% 47/569 [16:47<3:48:55, 26.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   8% 48/569 [17:03<3:19:27, 22.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   9% 49/569 [17:23<3:11:14, 22.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   9% 50/569 [17:40<2:59:25, 20.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   9% 51/569 [18:00<2:55:41, 20.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   9% 52/569 [18:26<3:12:08, 22.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   9% 53/569 [18:42<2:55:35, 20.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:   9% 54/569 [18:58<2:41:23, 18.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  10% 55/569 [19:15<2:38:14, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  10% 56/569 [19:30<2:28:58, 17.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  10% 57/569 [19:50<2:34:24, 18.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  10% 58/569 [20:05<2:26:28, 17.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  10% 59/569 [20:29<2:44:08, 19.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  11% 60/569 [20:44<2:32:24, 17.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  11% 61/569 [21:00<2:27:00, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  11% 62/569 [21:15<2:20:50, 16.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  11% 63/569 [21:33<2:22:52, 16.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  11% 64/569 [21:49<2:21:43, 16.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  11% 65/569 [22:07<2:22:56, 17.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  12% 66/569 [22:49<3:26:54, 24.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  12% 67/569 [23:32<4:11:44, 30.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  12% 68/569 [23:52<3:45:54, 27.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  12% 69/569 [24:15<3:35:39, 25.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  12% 70/569 [24:32<3:13:08, 23.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  12% 71/569 [24:47<2:52:56, 20.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  13% 72/569 [25:04<2:41:59, 19.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  13% 73/569 [25:20<2:32:38, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  13% 74/569 [25:35<2:23:54, 17.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  13% 75/569 [25:53<2:25:20, 17.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  13% 76/569 [26:36<3:27:05, 25.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  14% 77/569 [26:52<3:04:49, 22.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  14% 78/569 [27:16<3:07:59, 22.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  14% 79/569 [27:32<2:48:57, 20.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  14% 80/569 [27:49<2:41:33, 19.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  14% 81/569 [28:09<2:40:34, 19.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  14% 82/569 [28:32<2:48:26, 20.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  15% 83/569 [28:55<2:53:56, 21.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  15% 84/569 [29:10<2:38:44, 19.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  15% 85/569 [29:28<2:33:31, 19.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  15% 86/569 [29:44<2:25:36, 18.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  15% 87/569 [30:00<2:21:16, 17.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  15% 88/569 [30:15<2:14:15, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  16% 89/569 [30:35<2:20:50, 17.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  16% 90/569 [30:57<2:31:02, 18.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  16% 91/569 [31:12<2:22:24, 17.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  16% 92/569 [31:28<2:17:32, 17.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  16% 93/569 [31:43<2:11:34, 16.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  17% 94/569 [31:59<2:09:14, 16.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  17% 95/569 [32:18<2:16:14, 17.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  17% 96/569 [32:36<2:17:06, 17.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  17% 97/569 [32:54<2:17:42, 17.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  17% 98/569 [34:03<4:18:28, 32.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  17% 99/569 [34:20<3:42:23, 28.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  18% 100/569 [34:45<3:33:38, 27.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  18% 101/569 [35:05<3:14:18, 24.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  18% 102/569 [35:20<2:51:06, 21.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  18% 103/569 [35:37<2:40:49, 20.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  18% 104/569 [35:53<2:28:49, 19.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  18% 105/569 [36:08<2:19:35, 18.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  19% 106/569 [36:26<2:17:19, 17.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  19% 107/569 [36:41<2:11:42, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  19% 108/569 [37:05<2:25:50, 18.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  19% 109/569 [37:24<2:26:41, 19.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  19% 110/569 [37:40<2:18:59, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  20% 111/569 [38:07<2:37:54, 20.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  20% 112/569 [38:27<2:36:00, 20.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  20% 113/569 [38:46<2:33:41, 20.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  20% 114/569 [39:01<2:22:09, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  20% 115/569 [39:28<2:39:51, 21.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  20% 116/569 [39:45<2:30:48, 19.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  21% 117/569 [40:01<2:21:15, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  21% 118/569 [40:21<2:22:42, 18.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  21% 119/569 [40:41<2:24:34, 19.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  21% 120/569 [41:07<2:40:33, 21.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  21% 121/569 [41:22<2:25:10, 19.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  21% 122/569 [41:38<2:16:31, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  22% 123/569 [41:57<2:18:53, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  22% 124/569 [42:13<2:12:18, 17.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  22% 125/569 [42:29<2:07:13, 17.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  22% 126/569 [42:44<2:02:19, 16.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  22% 127/569 [43:04<2:10:14, 17.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  22% 128/569 [43:19<2:04:09, 16.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  23% 129/569 [43:37<2:05:17, 17.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  23% 130/569 [43:53<2:03:27, 16.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  23% 131/569 [44:10<2:02:59, 16.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  23% 132/569 [44:25<1:58:52, 16.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  23% 133/569 [44:41<1:57:32, 16.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  24% 134/569 [44:57<1:56:36, 16.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  24% 135/569 [45:15<1:59:53, 16.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  24% 136/569 [45:30<1:57:54, 16.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  24% 137/569 [45:48<2:01:01, 16.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  24% 138/569 [46:04<1:59:05, 16.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  24% 139/569 [46:24<2:06:22, 17.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  25% 140/569 [46:40<2:02:42, 17.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  25% 141/569 [47:00<2:07:22, 17.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  25% 142/569 [47:15<2:01:16, 17.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  25% 143/569 [47:31<1:58:45, 16.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  25% 144/569 [47:49<2:00:05, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  25% 145/569 [48:08<2:05:24, 17.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  26% 146/569 [48:24<2:01:12, 17.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  26% 147/569 [48:41<2:01:32, 17.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  26% 148/569 [48:57<1:58:10, 16.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  26% 149/569 [49:12<1:53:37, 16.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  26% 150/569 [49:28<1:51:51, 16.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  27% 151/569 [49:43<1:50:41, 15.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  27% 152/569 [50:05<2:03:05, 17.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  27% 153/569 [50:48<2:55:28, 25.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  27% 154/569 [51:04<2:35:58, 22.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  27% 155/569 [51:24<2:30:37, 21.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  27% 156/569 [51:42<2:22:06, 20.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  28% 157/569 [52:25<3:07:40, 27.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  28% 158/569 [52:40<2:41:11, 23.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  28% 159/569 [53:05<2:43:51, 23.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  28% 160/569 [53:21<2:26:42, 21.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  28% 161/569 [53:39<2:19:02, 20.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  28% 162/569 [53:55<2:11:01, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  29% 163/569 [54:15<2:10:48, 19.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  29% 164/569 [54:38<2:18:56, 20.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  29% 165/569 [54:53<2:07:36, 18.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  29% 166/569 [55:11<2:05:05, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  29% 167/569 [55:29<2:02:43, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  30% 168/569 [55:48<2:04:39, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  30% 169/569 [56:31<2:53:09, 25.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  30% 170/569 [56:46<2:30:49, 22.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  30% 171/569 [57:04<2:21:00, 21.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  30% 172/569 [57:20<2:09:10, 19.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  30% 173/569 [57:47<2:23:11, 21.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  31% 174/569 [58:03<2:11:36, 19.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  31% 175/569 [58:20<2:06:52, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  31% 176/569 [58:37<2:01:57, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  31% 177/569 [58:55<2:00:22, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  31% 178/569 [59:13<1:59:05, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  31% 179/569 [59:29<1:54:02, 17.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  32% 180/569 [59:44<1:48:10, 16.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  32% 181/569 [1:00:02<1:50:12, 17.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  32% 182/569 [1:00:18<1:47:47, 16.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  32% 183/569 [1:00:33<1:44:41, 16.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  32% 184/569 [1:01:42<3:26:36, 32.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  33% 185/569 [1:01:57<2:53:31, 27.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  33% 186/569 [1:02:14<2:33:38, 24.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  33% 187/569 [1:02:41<2:38:15, 24.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  33% 188/569 [1:02:57<2:21:13, 22.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  33% 189/569 [1:03:24<2:29:18, 23.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  33% 190/569 [1:03:39<2:13:06, 21.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  34% 191/569 [1:03:55<2:03:49, 19.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  34% 192/569 [1:04:11<1:56:26, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  34% 193/569 [1:04:26<1:49:21, 17.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  34% 194/569 [1:04:53<2:06:35, 20.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  34% 195/569 [1:05:36<2:48:17, 27.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  34% 196/569 [1:05:52<2:27:08, 23.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  35% 197/569 [1:06:10<2:15:51, 21.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  35% 198/569 [1:06:29<2:11:16, 21.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  35% 199/569 [1:06:49<2:08:02, 20.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  35% 200/569 [1:07:03<1:56:19, 18.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  35% 201/569 [1:07:19<1:50:15, 17.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  36% 202/569 [1:07:39<1:54:07, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  36% 203/569 [1:07:57<1:52:19, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  36% 204/569 [1:08:13<1:46:45, 17.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  36% 205/569 [1:08:30<1:46:17, 17.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  36% 206/569 [1:08:48<1:46:21, 17.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  36% 207/569 [1:09:03<1:42:08, 16.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  37% 208/569 [1:09:21<1:42:35, 17.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  37% 209/569 [1:09:36<1:39:21, 16.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  37% 210/569 [1:09:56<1:44:36, 17.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  37% 211/569 [1:10:11<1:40:44, 16.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  37% 212/569 [1:10:33<1:49:30, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  37% 213/569 [1:10:49<1:43:54, 17.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  38% 214/569 [1:11:06<1:44:01, 17.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  38% 215/569 [1:11:26<1:48:06, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  38% 216/569 [1:11:42<1:42:47, 17.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  38% 217/569 [1:12:00<1:43:08, 17.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  38% 218/569 [1:13:09<3:13:08, 33.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  38% 219/569 [1:13:26<2:45:41, 28.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  39% 220/569 [1:13:41<2:21:35, 24.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  39% 221/569 [1:13:57<2:05:48, 21.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  39% 222/569 [1:14:12<1:54:51, 19.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  39% 223/569 [1:14:28<1:46:59, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  39% 224/569 [1:14:43<1:41:17, 17.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  40% 225/569 [1:14:59<1:37:57, 17.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  40% 226/569 [1:15:22<1:48:07, 18.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  40% 227/569 [1:15:40<1:46:05, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  40% 228/569 [1:15:58<1:44:31, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  40% 229/569 [1:16:17<1:45:44, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  40% 230/569 [1:16:37<1:47:00, 18.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  41% 231/569 [1:17:20<2:26:50, 26.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  41% 232/569 [1:17:36<2:09:15, 23.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  41% 233/569 [1:17:52<1:57:14, 20.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  41% 234/569 [1:18:06<1:46:28, 19.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  41% 235/569 [1:18:22<1:39:35, 17.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  41% 236/569 [1:18:42<1:43:11, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  42% 237/569 [1:18:58<1:38:45, 17.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  42% 238/569 [1:20:08<3:04:31, 33.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  42% 239/569 [1:20:27<2:41:01, 29.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  42% 240/569 [1:20:44<2:19:34, 25.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  42% 241/569 [1:21:07<2:15:54, 24.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  43% 242/569 [1:21:23<2:00:14, 22.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  43% 243/569 [1:21:38<1:48:40, 20.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  43% 244/569 [1:21:55<1:43:03, 19.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  43% 245/569 [1:22:17<1:47:42, 19.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  43% 246/569 [1:22:44<1:58:28, 22.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  43% 247/569 [1:23:00<1:48:27, 20.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  44% 248/569 [1:23:19<1:47:10, 20.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  44% 249/569 [1:23:35<1:40:15, 18.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  44% 250/569 [1:23:51<1:35:01, 17.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  44% 251/569 [1:24:10<1:37:11, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  44% 252/569 [1:24:28<1:35:27, 18.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  44% 253/569 [1:24:52<1:44:28, 19.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  45% 254/569 [1:25:09<1:39:38, 18.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  45% 255/569 [1:25:25<1:34:44, 18.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  45% 256/569 [1:25:40<1:30:31, 17.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  45% 257/569 [1:25:57<1:28:15, 16.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  45% 258/569 [1:26:12<1:25:58, 16.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  46% 259/569 [1:26:32<1:30:58, 17.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  46% 260/569 [1:26:50<1:30:55, 17.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  46% 261/569 [1:27:09<1:33:19, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  46% 262/569 [1:27:27<1:32:23, 18.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  46% 263/569 [1:27:44<1:30:58, 17.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  46% 264/569 [1:28:04<1:33:10, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  47% 265/569 [1:28:22<1:32:13, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  47% 266/569 [1:29:05<2:09:11, 25.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  47% 267/569 [1:29:20<1:52:53, 22.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  47% 268/569 [1:29:36<1:43:43, 20.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  47% 269/569 [1:29:54<1:38:23, 19.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  47% 270/569 [1:31:03<2:52:18, 34.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  48% 271/569 [1:31:19<2:23:30, 28.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  48% 272/569 [1:31:35<2:04:27, 25.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  48% 273/569 [1:31:50<1:49:02, 22.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  48% 274/569 [1:32:05<1:38:13, 19.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  48% 275/569 [1:32:23<1:34:35, 19.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  49% 276/569 [1:32:43<1:35:05, 19.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  49% 277/569 [1:33:01<1:32:45, 19.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  49% 278/569 [1:33:17<1:27:52, 18.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  49% 279/569 [1:34:26<2:41:50, 33.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  49% 280/569 [1:34:41<2:14:19, 27.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  49% 281/569 [1:34:56<1:55:42, 24.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  50% 282/569 [1:35:12<1:42:49, 21.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  50% 283/569 [1:35:27<1:33:28, 19.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  50% 284/569 [1:35:44<1:29:25, 18.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  50% 285/569 [1:35:59<1:24:29, 17.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  50% 286/569 [1:36:15<1:20:53, 17.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  50% 287/569 [1:36:30<1:18:04, 16.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  51% 288/569 [1:36:47<1:18:25, 16.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  51% 289/569 [1:37:07<1:22:04, 17.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  51% 290/569 [1:37:29<1:27:52, 18.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  51% 291/569 [1:37:45<1:23:26, 18.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  51% 292/569 [1:38:04<1:24:56, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  51% 293/569 [1:38:21<1:22:03, 17.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  52% 294/569 [1:38:36<1:18:23, 17.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  52% 295/569 [1:39:19<1:53:24, 24.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  52% 296/569 [1:39:34<1:39:46, 21.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  52% 297/569 [1:39:50<1:31:53, 20.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  52% 298/569 [1:40:33<2:01:51, 26.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  53% 299/569 [1:40:50<1:48:21, 24.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  53% 300/569 [1:41:33<2:13:04, 29.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  53% 301/569 [1:41:48<1:53:19, 25.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  53% 302/569 [1:42:05<1:41:43, 22.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  53% 303/569 [1:42:20<1:30:56, 20.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  53% 304/569 [1:42:38<1:27:11, 19.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  54% 305/569 [1:43:00<1:29:54, 20.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  54% 306/569 [1:43:16<1:23:15, 19.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  54% 307/569 [1:43:34<1:22:01, 18.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  54% 308/569 [1:43:52<1:20:36, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  54% 309/569 [1:44:08<1:16:48, 17.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  54% 310/569 [1:44:24<1:13:29, 17.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  55% 311/569 [1:45:07<1:46:44, 24.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  55% 312/569 [1:45:24<1:36:19, 22.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  55% 313/569 [1:45:50<1:41:23, 23.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  55% 314/569 [1:46:10<1:35:17, 22.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  55% 315/569 [1:46:52<2:00:50, 28.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  56% 316/569 [1:47:12<1:49:06, 25.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  56% 317/569 [1:47:32<1:40:36, 23.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  56% 318/569 [1:48:14<2:03:54, 29.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  56% 319/569 [1:48:30<1:46:15, 25.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  56% 320/569 [1:48:45<1:32:46, 22.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  56% 321/569 [1:49:01<1:23:49, 20.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  57% 322/569 [1:49:16<1:17:38, 18.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  57% 323/569 [1:49:32<1:13:32, 17.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  57% 324/569 [1:49:47<1:10:08, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  57% 325/569 [1:50:06<1:11:03, 17.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  57% 326/569 [1:51:15<2:13:44, 33.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  57% 327/569 [1:51:40<2:03:26, 30.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  58% 328/569 [1:52:02<1:52:23, 27.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  58% 329/569 [1:52:45<2:09:54, 32.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  58% 330/569 [1:53:01<1:49:26, 27.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  58% 331/569 [1:53:16<1:34:18, 23.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  58% 332/569 [1:53:33<1:26:47, 21.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  59% 333/569 [1:53:49<1:18:56, 20.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  59% 334/569 [1:54:12<1:22:24, 21.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  59% 335/569 [1:54:28<1:15:59, 19.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  59% 336/569 [1:54:45<1:12:37, 18.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  59% 337/569 [1:55:01<1:09:05, 17.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  59% 338/569 [1:55:21<1:10:48, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  60% 339/569 [1:56:30<2:08:47, 33.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  60% 340/569 [1:56:55<1:58:15, 30.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  60% 341/569 [1:57:15<1:45:31, 27.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  60% 342/569 [1:57:33<1:33:53, 24.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  60% 343/569 [1:57:57<1:32:33, 24.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  60% 344/569 [1:58:12<1:21:40, 21.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  61% 345/569 [1:59:22<2:14:43, 36.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  61% 346/569 [1:59:38<1:52:12, 30.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  61% 347/569 [1:59:54<1:35:50, 25.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  61% 348/569 [2:00:10<1:25:02, 23.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  61% 349/569 [2:00:28<1:18:23, 21.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  62% 350/569 [2:01:37<2:10:48, 35.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  62% 351/569 [2:01:58<1:53:10, 31.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  62% 352/569 [2:02:13<1:35:20, 26.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  62% 353/569 [2:02:33<1:28:10, 24.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  62% 354/569 [2:02:51<1:20:29, 22.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  62% 355/569 [2:03:09<1:15:50, 21.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  63% 356/569 [2:03:25<1:10:05, 19.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  63% 357/569 [2:03:43<1:07:52, 19.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  63% 358/569 [2:04:01<1:06:13, 18.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  63% 359/569 [2:04:21<1:06:31, 19.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  63% 360/569 [2:04:37<1:03:09, 18.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  63% 361/569 [2:04:54<1:02:01, 17.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  64% 362/569 [2:05:21<1:10:55, 20.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  64% 363/569 [2:05:37<1:06:20, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  64% 364/569 [2:05:53<1:01:56, 18.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  64% 365/569 [2:06:09<59:54, 17.62s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  64% 366/569 [2:06:27<59:27, 17.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  64% 367/569 [2:07:36<1:51:30, 33.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  65% 368/569 [2:07:52<1:34:07, 28.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  65% 369/569 [2:08:08<1:21:32, 24.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  65% 370/569 [2:08:28<1:16:13, 22.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  65% 371/569 [2:08:44<1:08:49, 20.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  65% 372/569 [2:09:53<1:56:37, 35.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  66% 373/569 [2:10:09<1:36:01, 29.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  66% 374/569 [2:10:24<1:22:03, 25.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  66% 375/569 [2:10:39<1:11:44, 22.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  66% 376/569 [2:10:55<1:05:25, 20.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  66% 377/569 [2:11:22<1:11:11, 22.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  66% 378/569 [2:11:38<1:04:58, 20.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  67% 379/569 [2:12:05<1:10:34, 22.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  67% 380/569 [2:12:30<1:12:45, 23.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  67% 381/569 [2:13:12<1:30:55, 29.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  67% 382/569 [2:13:30<1:19:58, 25.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  67% 383/569 [2:13:45<1:09:39, 22.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  67% 384/569 [2:14:03<1:04:29, 20.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  68% 385/569 [2:14:22<1:02:50, 20.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  68% 386/569 [2:14:37<57:33, 18.87s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  68% 387/569 [2:14:55<55:56, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  68% 388/569 [2:15:38<1:18:02, 25.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  68% 389/569 [2:15:53<1:07:59, 22.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  69% 390/569 [2:16:13<1:04:53, 21.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  69% 391/569 [2:16:31<1:01:27, 20.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  69% 392/569 [2:16:46<56:26, 19.13s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  69% 393/569 [2:17:01<52:30, 17.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  69% 394/569 [2:17:19<52:04, 17.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  69% 395/569 [2:18:28<1:36:32, 33.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  70% 396/569 [2:18:44<1:20:41, 27.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  70% 397/569 [2:19:27<1:33:00, 32.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  70% 398/569 [2:19:45<1:20:10, 28.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  70% 399/569 [2:20:03<1:11:18, 25.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  70% 400/569 [2:20:27<1:09:20, 24.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  70% 401/569 [2:20:46<1:04:44, 23.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  71% 402/569 [2:21:11<1:05:55, 23.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  71% 403/569 [2:21:26<58:19, 21.08s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  71% 404/569 [2:21:46<56:55, 20.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  71% 405/569 [2:22:03<53:13, 19.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  71% 406/569 [2:22:18<49:20, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  72% 407/569 [2:22:36<48:49, 18.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  72% 408/569 [2:22:51<45:59, 17.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  72% 409/569 [2:23:08<46:10, 17.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  72% 410/569 [2:23:28<48:08, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  72% 411/569 [2:23:53<53:07, 20.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  72% 412/569 [2:24:11<50:34, 19.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  73% 413/569 [2:24:27<48:02, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  73% 414/569 [2:24:47<49:02, 18.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  73% 415/569 [2:25:03<45:57, 17.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  73% 416/569 [2:25:21<45:38, 17.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  73% 417/569 [2:25:40<46:29, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  73% 418/569 [2:25:55<43:50, 17.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  74% 419/569 [2:26:11<42:06, 16.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  74% 420/569 [2:26:26<40:55, 16.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  74% 421/569 [2:26:43<40:37, 16.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  74% 422/569 [2:26:59<39:57, 16.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  74% 423/569 [2:27:14<38:58, 16.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  75% 424/569 [2:27:30<38:23, 15.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  75% 425/569 [2:27:50<41:24, 17.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  75% 426/569 [2:28:10<42:52, 17.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  75% 427/569 [2:28:28<42:31, 17.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  75% 428/569 [2:28:44<40:47, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  75% 429/569 [2:28:59<39:11, 16.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  76% 430/569 [2:29:17<39:57, 17.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  76% 431/569 [2:29:42<44:22, 19.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  76% 432/569 [2:29:57<41:26, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  76% 433/569 [2:30:22<45:44, 20.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  76% 434/569 [2:31:05<1:00:37, 26.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  76% 435/569 [2:31:28<57:48, 25.88s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  77% 436/569 [2:31:43<49:55, 22.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  77% 437/569 [2:31:58<44:36, 20.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  77% 438/569 [2:32:20<45:18, 20.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  77% 439/569 [2:32:37<43:02, 19.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  77% 440/569 [2:32:54<40:53, 19.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  78% 441/569 [2:34:04<1:12:49, 34.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  78% 442/569 [2:34:22<1:01:59, 29.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  78% 443/569 [2:34:38<52:57, 25.22s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  78% 444/569 [2:34:54<47:00, 22.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  78% 445/569 [2:35:10<42:37, 20.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  78% 446/569 [2:35:26<39:15, 19.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  79% 447/569 [2:36:35<1:09:27, 34.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  79% 448/569 [2:36:50<57:21, 28.44s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  79% 449/569 [2:37:08<50:44, 25.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  79% 450/569 [2:37:26<45:52, 23.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  79% 451/569 [2:38:35<1:12:36, 36.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  79% 452/569 [2:38:53<1:00:43, 31.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  80% 453/569 [2:39:15<54:52, 28.38s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  80% 454/569 [2:39:33<48:30, 25.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  80% 455/569 [2:39:48<42:07, 22.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  80% 456/569 [2:40:58<1:08:39, 36.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  80% 457/569 [2:41:13<56:19, 30.18s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  80% 458/569 [2:41:29<47:42, 25.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  81% 459/569 [2:41:44<41:17, 22.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  81% 460/569 [2:42:00<37:41, 20.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  81% 461/569 [2:42:20<36:56, 20.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  81% 462/569 [2:42:38<35:05, 19.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  81% 463/569 [2:42:58<34:57, 19.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  82% 464/569 [2:43:41<46:42, 26.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  82% 465/569 [2:43:57<40:59, 23.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  82% 466/569 [2:44:12<36:13, 21.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  82% 467/569 [2:44:28<32:56, 19.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  82% 468/569 [2:44:44<30:53, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  82% 469/569 [2:45:02<30:20, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  83% 470/569 [2:45:20<29:54, 18.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  83% 471/569 [2:45:39<30:19, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  83% 472/569 [2:46:49<54:49, 33.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  83% 473/569 [2:47:11<48:34, 30.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  83% 474/569 [2:47:28<41:56, 26.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  83% 475/569 [2:47:43<36:05, 23.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  84% 476/569 [2:48:08<36:37, 23.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  84% 477/569 [2:48:32<36:23, 23.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  84% 478/569 [2:48:56<36:05, 23.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  84% 479/569 [2:49:14<33:04, 22.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  84% 480/569 [2:49:34<31:33, 21.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  85% 481/569 [2:49:49<28:27, 19.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  85% 482/569 [2:50:04<26:20, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  85% 483/569 [2:50:22<26:03, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  85% 484/569 [2:50:39<25:02, 17.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  85% 485/569 [2:50:54<23:40, 16.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  85% 486/569 [2:51:37<34:04, 24.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  86% 487/569 [2:51:56<31:31, 23.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  86% 488/569 [2:52:14<29:02, 21.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  86% 489/569 [2:52:32<27:16, 20.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  86% 490/569 [2:52:47<24:53, 18.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  86% 491/569 [2:53:05<24:12, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  86% 492/569 [2:53:23<23:33, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  87% 493/569 [2:53:41<23:01, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  87% 494/569 [2:54:00<23:13, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  87% 495/569 [2:54:15<21:41, 17.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  87% 496/569 [2:54:58<30:33, 25.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  87% 497/569 [2:55:18<28:10, 23.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  88% 498/569 [2:55:36<25:48, 21.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  88% 499/569 [2:55:54<24:04, 20.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  88% 500/569 [2:56:09<21:51, 19.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  88% 501/569 [2:56:27<21:10, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  88% 502/569 [2:56:44<20:28, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  88% 503/569 [2:57:00<19:20, 17.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  89% 504/569 [2:57:18<19:04, 17.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  89% 505/569 [2:58:01<26:55, 25.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  89% 506/569 [2:58:17<23:43, 22.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  89% 507/569 [2:58:33<21:22, 20.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  89% 508/569 [2:58:49<19:26, 19.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  89% 509/569 [2:59:08<19:16, 19.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  90% 510/569 [2:59:25<18:10, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  90% 511/569 [2:59:40<16:52, 17.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  90% 512/569 [3:00:02<17:53, 18.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  90% 513/569 [3:00:22<17:56, 19.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  90% 514/569 [3:00:37<16:29, 17.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  91% 515/569 [3:00:53<15:37, 17.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  91% 516/569 [3:01:09<14:49, 16.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  91% 517/569 [3:01:51<21:16, 24.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  91% 518/569 [3:02:07<18:33, 21.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  91% 519/569 [3:02:22<16:29, 19.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  91% 520/569 [3:03:31<28:18, 34.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  92% 521/569 [3:03:47<23:03, 28.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  92% 522/569 [3:04:07<20:31, 26.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  92% 523/569 [3:04:23<17:44, 23.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  92% 524/569 [3:04:48<17:46, 23.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  92% 525/569 [3:05:03<15:34, 21.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  92% 526/569 [3:06:13<25:35, 35.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  93% 527/569 [3:07:22<32:05, 45.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  93% 528/569 [3:07:40<25:31, 37.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  93% 529/569 [3:07:59<21:21, 32.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  93% 530/569 [3:08:15<17:35, 27.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  93% 531/569 [3:08:32<15:18, 24.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  93% 532/569 [3:08:50<13:44, 22.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  94% 533/569 [3:09:08<12:34, 20.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  94% 534/569 [3:09:28<12:00, 20.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  94% 535/569 [3:09:43<10:44, 18.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  94% 536/569 [3:10:01<10:15, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  94% 537/569 [3:10:43<13:45, 25.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  95% 538/569 [3:10:59<11:49, 22.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  95% 539/569 [3:11:15<10:21, 20.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  95% 540/569 [3:11:30<09:15, 19.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  95% 541/569 [3:11:49<08:49, 18.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  95% 542/569 [3:12:09<08:39, 19.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  95% 543/569 [3:12:26<08:05, 18.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  96% 544/569 [3:12:42<07:26, 17.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  96% 545/569 [3:13:00<07:11, 17.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  96% 546/569 [3:13:15<06:32, 17.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  96% 547/569 [3:13:30<06:00, 16.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  96% 548/569 [3:14:40<11:19, 32.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  96% 549/569 [3:15:05<10:04, 30.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  97% 550/569 [3:15:21<08:10, 25.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  97% 551/569 [3:15:37<06:51, 22.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  97% 552/569 [3:15:54<06:01, 21.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  97% 553/569 [3:16:37<07:25, 27.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  97% 554/569 [3:16:53<06:01, 24.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  98% 555/569 [3:17:09<05:04, 21.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  98% 556/569 [3:17:24<04:18, 19.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  98% 557/569 [3:17:40<03:42, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  98% 558/569 [3:18:05<03:45, 20.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  98% 559/569 [3:18:21<03:11, 19.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  98% 560/569 [3:18:36<02:41, 17.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  99% 561/569 [3:18:56<02:27, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  99% 562/569 [3:19:20<02:21, 20.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  99% 563/569 [3:19:35<01:51, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  99% 564/569 [3:20:44<02:49, 33.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  99% 565/569 [3:21:00<01:53, 28.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating:  99% 566/569 [3:21:18<01:15, 25.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating: 100% 567/569 [3:21:33<00:44, 22.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating: 100% 568/569 [3:21:50<00:20, 20.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Evaluating: 100% 569/569 [3:22:59<00:00, 21.41s/it]\n",
            "Loaded and validated 569 test samples.\n",
            "Loading base model and adapters...\n",
            "Model loaded successfully.\n",
            "Running inference on test set...\n",
            " PowerShell-Sentinel Final Evaluation  \n",
            "                Report                 \n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
            "┃                     Metric ┃ Score  ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
            "│              Total Samples │ 569    │\n",
            "│        Parse Success Count │ 531    │\n",
            "│        Parse Failure Count │ 38     │\n",
            "│    JSON Parse Success Rate │ 93.32% │\n",
            "│     Deobfuscation Accuracy │ 72.50% │\n",
            "│    Intent F1-Score (Macro) │ 70.08% │\n",
            "│ MITRE TTP F1-Score (Macro) │ 70.08% │\n",
            "└────────────────────────────┴────────┘\n",
            "\n",
            "\n",
            "✅ ✅ ✅ FINAL EVALUATION COMPLETE AND REPORT BACKED UP TO DRIVE. ✅ ✅ ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (Obsolete) 8. CREATE FINAL GGUF DELIVERABLE ---\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import os\n",
        "import sys\n",
        "import site\n",
        "\n",
        "# --- Step 1: Define Paths ---\n",
        "base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "adapter_path = \"/content/powershell-sentinel-main/models/final_model/final_checkpoint\"\n",
        "merged_model_path = \"/content/merged_final_model\"\n",
        "gguf_output_path = \"/content/powershell_sentinel_llama3_q4km.gguf\"\n",
        "drive_gguf_path = f\"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/powershell_sentinel_llama3_q4km.gguf\"\n",
        "\n",
        "# --- Step 2: Load and Merge the Model (No Changes Here) ---\n",
        "compute_dtype = getattr(torch, \"bfloat16\")\n",
        "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n",
        "\n",
        "print(\"Loading base model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name, quantization_config=quant_config, device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "print(\"Loading adapters and merging...\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model = model.merge_and_unload()\n",
        "print(\"Model merged successfully.\")\n",
        "\n",
        "print(f\"Saving merged model to {merged_model_path}...\")\n",
        "model.save_pretrained(merged_model_path)\n",
        "tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "# --- Step 3: Forceful Installation of llama-cpp-python from Source ---\n",
        "# This is the definitive fix. It guarantees the convert.py script is available.\n",
        "print(\"\\nInstalling llama-cpp-python from source...\")\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --no-cache-dir --force-reinstall llama-cpp-python -q\n",
        "\n",
        "# --- Step 4: GGUF Conversion (No Changes Here, but it will now work) ---\n",
        "# We use a more robust method to find the script, just in case\n",
        "CONVERT_SCRIPT_PATH = None\n",
        "for path in site.getsitepackages():\n",
        "    if os.path.exists(os.path.join(path, 'llama_cpp', 'convert.py')):\n",
        "        CONVERT_SCRIPT_PATH = os.path.join(path, 'llama_cpp', 'convert.py')\n",
        "        break\n",
        "\n",
        "if CONVERT_SCRIPT_PATH:\n",
        "    print(f\"Found conversion script at {CONVERT_SCRIPT_PATH}\")\n",
        "    print(\"Starting GGUF conversion...\")\n",
        "\n",
        "    !python {CONVERT_SCRIPT_PATH} {merged_model_path} --outfile {gguf_output_path} --outtype q4_k_m\n",
        "\n",
        "    print(f\"\\n\\n✅ GGUF conversion complete! File at: {gguf_output_path}\")\n",
        "\n",
        "    # --- Step 5: Backup the final GGUF to Google Drive ---\n",
        "    !cp {gguf_output_path} {drive_gguf_path}\n",
        "    print(f\"✅ Final GGUF model backed up to: {drive_gguf_path}\")\n",
        "else:\n",
        "    print(\"❌ CRITICAL ERROR: Could not find llama.cpp convert.py script even after source installation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434,
          "referenced_widgets": [
            "13900b6dc92e4e599169f89b4254a351",
            "32da7a0dfc88443fb1f2406d62b87e77",
            "32a2195a9783432f91755842cff3a12e",
            "d6941731b17446f88513257bb15e7998",
            "66c856bd16a345f58d65b1e5b338e148",
            "c53c7e6c6c6142b4a6fa946424d0265e",
            "c7430be53b4f428e9141d4c4ad3b7f3c",
            "c8a78fc9dcb642b6a66a044291375656",
            "51ae85a97fa54c9ea04d3ab1f1247cc2",
            "6e625429b3d542faab874f1b717786a6",
            "acb2615513884286a4673c1a31b91be9"
          ]
        },
        "id": "4-kiDijFVsrB",
        "outputId": "89cc674b-837a-41e7-f91b-71b62c8f41f4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13900b6dc92e4e599169f89b4254a351"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading adapters and merging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model merged successfully.\n",
            "Saving merged model to /content/merged_final_model...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "❌ Could not find llama.cpp convert.py script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (Current) 8. CREATE FINAL GGUF DELIVERABLE ---\n",
        "# ==============================================================================\n",
        "# DEFINITIVE SCRIPT: CREATE & SAVE ALL GGUF MODELS (Consolidated Version)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. SETUP AND PATH DEFINITIONS ---\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# THE CRITICAL FIX: Mount Google Drive at the very beginning to ensure it's connected.\n",
        "print(\"--- Mounting Google Drive ---\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"✅ Google Drive mounted successfully.\")\n",
        "\n",
        "# Define all necessary paths\n",
        "base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "drive_backup_path = \"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/final_model_adapters.zip\"\n",
        "local_restore_dir = \"/content/restored_model_temp\"\n",
        "merged_model_path = \"/content/merged_model_final\" # Absolute path\n",
        "llama_cpp_path = \"/content/llama.cpp\"\n",
        "\n",
        "# Define the base name for the GGUF files\n",
        "gguf_base_name = \"powershell_sentinel_llama3\"\n",
        "intermediate_gguf_path = f\"/content/{gguf_base_name}_f16.gguf\"\n",
        "\n",
        "# Define the list of quantization \"flavours\" to create\n",
        "quantization_types = [\n",
        "    \"Q8_0\", \"Q6_K\", \"Q5_K_M\", \"Q4_K_M\", \"Q3_K_M\", \"Q2_K\"\n",
        "]\n",
        "print(f\"\\n✅ Will generate {len(quantization_types)} GGUF models: {', '.join(quantization_types)}\")\n",
        "\n",
        "# --- 2. THE \"CLEAN MERGE\" WORKFLOW ---\n",
        "print(f\"\\nRestoring fine-tuned model adapters from: {drive_backup_path}\")\n",
        "# Ensure the restore directory is clean for idempotent runs\n",
        "!rm -rf {local_restore_dir}\n",
        "!mkdir -p {local_restore_dir}\n",
        "!unzip -o -q \"{drive_backup_path}\" -d {local_restore_dir}\n",
        "adapter_path = next((os.path.join(root, d) for root, dirs, _ in os.walk(local_restore_dir) for d in dirs if \"final_checkpoint\" in d), None)\n",
        "if not adapter_path:\n",
        "    raise FileNotFoundError(\"Could not find 'final_checkpoint' directory.\")\n",
        "print(f\"Found adapter checkpoint at: {adapter_path}\")\n",
        "\n",
        "print(\"\\nLoading base model in bfloat16 for a clean merge...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "print(\"✅ Base model loaded successfully.\")\n",
        "\n",
        "print(\"\\nLoading PEFT adapters and merging...\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path).merge_and_unload()\n",
        "print(\"✅ Model and adapters merged successfully.\")\n",
        "\n",
        "print(f\"\\nSaving the clean, high-precision merged model to: {merged_model_path}\")\n",
        "model.save_pretrained(merged_model_path)\n",
        "tokenizer.save_pretrained(merged_model_path)\n",
        "del model, base_model\n",
        "torch.cuda.empty_cache()\n",
        "print(\"✅ Merged model saved and GPU memory cleaned.\")\n",
        "\n",
        "# --- 3. CLONE LLAMA.CPP AND INSTALL REQUIREMENTS ---\n",
        "print(\"\\nCloning llama.cpp repository and installing dependencies...\")\n",
        "# Only clone if the directory doesn't exist to save time on reruns\n",
        "if not os.path.exists(llama_cpp_path):\n",
        "    !git clone https://github.com/ggerganov/llama.cpp.git {llama_cpp_path}\n",
        "!pip install -r {os.path.join(llama_cpp_path, 'requirements.txt')} -q\n",
        "print(\"✅ llama.cpp is ready and dependencies are installed.\")\n",
        "\n",
        "# --- 4. GGUF CREATION - THE CORRECT TWO-STEP PROCESS ---\n",
        "%cd {llama_cpp_path}\n",
        "print(f\"\\nChanged working directory to: {os.getcwd()}\")\n",
        "\n",
        "# STEP 4A: Convert the clean HF model to a base f16 GGUF file.\n",
        "print(\"\\n--- Step 4A: Converting to initial f16 GGUF format ---\")\n",
        "!python convert_hf_to_gguf.py \"{merged_model_path}\" \\\n",
        "  --outfile \"{intermediate_gguf_path}\" \\\n",
        "  --outtype f16\n",
        "if not os.path.exists(intermediate_gguf_path):\n",
        "    raise RuntimeError(\"Failed to create the intermediate f16 GGUF file.\")\n",
        "print(\"✅ Intermediate f16 GGUF file created successfully.\")\n",
        "\n",
        "# STEP 4B: Compile the C++ tools using CMake.\n",
        "print(\"\\n--- Step 4B: Compiling C++ tools using CMake ---\")\n",
        "!cmake -B build\n",
        "!cmake --build build --config Release\n",
        "\n",
        "# Correct executable name based on build logs\n",
        "quantize_executable_name = \"llama-quantize\"\n",
        "quantize_executable_path = os.path.join(os.getcwd(), \"build\", \"bin\", quantize_executable_name)\n",
        "if not os.path.exists(quantize_executable_path):\n",
        "    raise RuntimeError(f\"Failed to compile '{quantize_executable_name}'.\")\n",
        "print(f\"✅ '{quantize_executable_name}' tool compiled successfully.\")\n",
        "\n",
        "# STEP 4C: Loop through the quantization types and create each GGUF file.\n",
        "print(\"\\n--- Step 4C: Quantizing to multiple formats ---\")\n",
        "for quant_type in quantization_types:\n",
        "    final_gguf_path_local = f\"/content/{gguf_base_name}_{quant_type.lower()}.gguf\"\n",
        "    print(f\"\\n--- Quantizing to {quant_type} ---\")\n",
        "\n",
        "    # Run the quantization command\n",
        "    !{quantize_executable_path} \"{intermediate_gguf_path}\" \"{final_gguf_path_local}\" {quant_type}\n",
        "\n",
        "    # Verify and backup each file as it's created\n",
        "    if os.path.exists(final_gguf_path_local):\n",
        "        file_size = os.path.getsize(final_gguf_path_local) / (1024**3)\n",
        "        print(f\"✅ {quant_type} GGUF created successfully! Size: {file_size:.2f} GB\")\n",
        "\n",
        "        # Backup to Google Drive\n",
        "        drive_gguf_path = f\"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/{os.path.basename(final_gguf_path_local)}\"\n",
        "        !cp \"{final_gguf_path_local}\" \"{drive_gguf_path}\"\n",
        "        print(f\"   -> Backed up to: {drive_gguf_path}\")\n",
        "    else:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to create the {quant_type} GGUF file.\")\n",
        "\n",
        "# --- 5. FINAL VERIFICATION ---\n",
        "%cd /content\n",
        "print(f\"\\n\\n--- FINAL SUMMARY ---\")\n",
        "print(\"All quantization tasks are complete. Verifying files in Google Drive:\")\n",
        "for quant_type in quantization_types:\n",
        "    drive_path = f\"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/{gguf_base_name}_{quant_type.lower()}.gguf\"\n",
        "    if os.path.exists(drive_path):\n",
        "        print(f\"  - ✅ {os.path.basename(drive_path)}\")\n",
        "    else:\n",
        "        print(f\"  - ❌ {os.path.basename(drive_path)} (Missing from Drive!)\")\n",
        "\n",
        "# --- 6. FINAL CLEANUP ---\n",
        "print(\"\\nCleaning up all temporary files and directories...\")\n",
        "!rm -rf {local_restore_dir}\n",
        "!rm -rf {merged_model_path}\n",
        "!rm -rf {llama_cpp_path}\n",
        "!rm -f {intermediate_gguf_path} # Remove the large intermediate file\n",
        "# Also remove the local copies of the final GGUFs now that they are backed up\n",
        "!rm -f /content/*.gguf\n",
        "print(\"✅ Cleanup complete. Script finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0c985971cac242ca9b205736d3990a29",
            "56cfd9e819bf4e088b1ed74063406aea",
            "66bc806d3dc44d86b1278fd76566e3fb",
            "3722a7fd63ab4238a20b548fa943d16b",
            "66dc2f85dddb4c8ebc276008e1906a93",
            "aaf1cfd9ec1f4defb15ffa030d7a1e52",
            "81c794f698b54b018f503ed222970377",
            "0af758fa77d146d988cf542a3001b0b4",
            "e33e03b165ad4a01bcdf5e02822ab54c",
            "03bd934e23be47c7ac3ddc40f0ed51f2",
            "160f20777805408eb9f56663f6f61637"
          ]
        },
        "id": "JGjyBUkvHS_q",
        "outputId": "1cbf5eed-3146-46a7-85d6-04173ce58fd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Mounting Google Drive ---\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive mounted successfully.\n",
            "\n",
            "✅ Will generate 6 GGUF models: Q8_0, Q6_K, Q5_K_M, Q4_K_M, Q3_K_M, Q2_K\n",
            "\n",
            "Restoring fine-tuned model adapters from: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/final_model_adapters.zip\n",
            "Found adapter checkpoint at: /content/restored_model_temp/content/powershell-sentinel-main/models/final_model/final_checkpoint\n",
            "\n",
            "Loading base model in bfloat16 for a clean merge...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c985971cac242ca9b205736d3990a29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Base model loaded successfully.\n",
            "\n",
            "Loading PEFT adapters and merging...\n",
            "✅ Model and adapters merged successfully.\n",
            "\n",
            "Saving the clean, high-precision merged model to: /content/merged_model_final\n",
            "✅ Merged model saved and GPU memory cleaned.\n",
            "\n",
            "Cloning llama.cpp repository and installing dependencies...\n",
            "Cloning into '/content/llama.cpp'...\n",
            "remote: Enumerating objects: 58777, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 58777 (delta 66), reused 23 (delta 23), pack-reused 58662 (from 4)\u001b[K\n",
            "Receiving objects: 100% (58777/58777), 142.76 MiB | 36.06 MiB/s, done.\n",
            "Resolving deltas: 100% (42596/42596), done.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "shap 0.48.0 requires numba>=0.54, which is not installed.\n",
            "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "gradio 5.41.0 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ llama.cpp is ready and dependencies are installed.\n",
            "/content/llama.cpp\n",
            "\n",
            "Changed working directory to: /content/llama.cpp\n",
            "\n",
            "--- Step 4A: Converting to initial f16 GGUF format ---\n",
            "INFO:hf-to-gguf:Loading model: merged_model_final\n",
            "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> F16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> F16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 8192\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "2025-08-11 19:19:01.573650: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-11 19:19:02.329248: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754939942.646702   76914 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754939942.729413   76914 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754939943.371683   76914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754939943.371720   76914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754939943.371723   76914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754939943.371726   76914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-11 19:19:03.428778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128009\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
            "\n",
            "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "' }}{% endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/powershell_sentinel_llama3_f16.gguf: n_tensors = 291, total_size = 16.1G\n",
            "Writing: 100% 16.1G/16.1G [02:18<00:00, 116Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/powershell_sentinel_llama3_f16.gguf\n",
            "✅ Intermediate f16 GGUF file created successfully.\n",
            "\n",
            "--- Step 4B: Compiling C++ tools using CMake ---\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.6134\n",
            "-- ggml commit:  be48528b\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.9s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  4%] Built target ggml-base\n",
            "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 11%] Built target ggml-cpu\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 11%] Built target ggml\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 22%] Built target llama\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 22%] Built target build_info\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 28%] Built target common\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 28%] Built target test-tokenizer-0\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 29%] Built target test-sampling\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 30%] Built target test-grammar-parser\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 31%] Built target test-grammar-integration\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 32%] Built target test-llama-grammar\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 33%] Built target test-chat\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 35%] Built target test-json-schema-to-grammar\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 36%] Built target test-quantize-stats\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 36%] Built target test-gbnf-validator\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 37%] Built target test-tokenizer-1-bpe\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 38%] Built target test-tokenizer-1-spm\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 39%] Built target test-chat-parser\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 41%] Built target test-chat-template\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 42%] Built target test-json-partial\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 43%] Built target test-log\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 44%] Built target test-regex-partial\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 46%] Built target test-thread-safety\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 47%] Built target test-arg-parser\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 49%] Built target test-gguf\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 50%] Built target test-backend-ops\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 51%] Built target test-model-load-cancel\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 53%] Built target test-autorelease\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 54%] Built target test-barrier\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 56%] Built target test-quantize-fns\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 57%] Built target test-quantize-perf\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 58%] Built target test-rope\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 60%] Built target mtmd\n",
            "[ 61%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 61%] Built target test-mtmd-c-api\n",
            "[ 61%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 62%] Built target test-c\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 63%] Built target llama-batched\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 63%] Built target llama-embedding\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 64%] Built target llama-eval-callback\n",
            "[ 64%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 64%] Built target sha256\n",
            "[ 65%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 65%] Built target xxhash\n",
            "[ 66%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 66%] Built target sha1\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 67%] Built target llama-gguf-hash\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 68%] Built target llama-gguf\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 69%] Built target llama-gritlm\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 70%] Built target llama-lookahead\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 71%] Built target llama-lookup\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 72%] Built target llama-lookup-create\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 73%] Built target llama-lookup-merge\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 73%] Built target llama-lookup-stats\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 74%] Built target llama-parallel\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 75%] Built target llama-passkey\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 76%] Built target llama-retrieval\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 77%] Built target llama-save-load-state\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 77%] Built target llama-simple\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 78%] Built target llama-simple-chat\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 79%] Built target llama-speculative\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 80%] Built target llama-speculative-simple\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 80%] Built target llama-gen-docs\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 81%] Built target llama-finetune\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 82%] Built target llama-diffusion-cli\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 83%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 84%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 84%] Built target llama-vdot\n",
            "[ 84%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 84%] Built target llama-q8dot\n",
            "[ 84%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 84%] Built target llama-batched-bench\n",
            "[ 84%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 85%] Built target llama-gguf-split\n",
            "[ 85%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 86%] Built target llama-imatrix\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 87%] Built target llama-bench\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 88%] Built target llama-cli\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 89%] Built target llama-perplexity\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 90%] Built target llama-quantize\n",
            "[ 91%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 91%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 92%] Built target llama-server\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 93%] Built target llama-run\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 94%] Built target llama-tokenize\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 94%] Built target llama-tts\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 94%] Built target llama-llava-cli\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 95%] Built target llama-gemma3-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 96%] Built target llama-minicpmv-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 97%] Built target llama-qwen2vl-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 98%] Built target llama-mtmd-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 99%] Built target llama-cvector-generator\n",
            "[100%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[100%] Built target llama-export-lora\n",
            "✅ 'llama-quantize' tool compiled successfully.\n",
            "\n",
            "--- Step 4C: Quantizing to multiple formats ---\n",
            "\n",
            "--- Quantizing to Q8_0 ---\n",
            "main: build = 6134 (be48528b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/powershell_sentinel_llama3_f16.gguf' to '/content/powershell_sentinel_llama3_q8_0.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 291 tensors from /content/powershell_sentinel_llama3_f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3 8B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Meta-Llama-3\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q8_0 .. size =  1002.00 MiB ->   532.31 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q8_0 .. size =  1002.00 MiB ->   532.31 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.02 MB\n",
            "llama_model_quantize_impl: quant size  =  8137.64 MB\n",
            "\n",
            "main: quantize time = 93559.69 ms\n",
            "main:    total time = 93559.69 ms\n",
            "✅ Q8_0 GGUF created successfully! Size: 7.95 GB\n",
            "   -> Backed up to: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/powershell_sentinel_llama3_q8_0.gguf\n",
            "\n",
            "--- Quantizing to Q6_K ---\n",
            "main: build = 6134 (be48528b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/powershell_sentinel_llama3_f16.gguf' to '/content/powershell_sentinel_llama3_q6_k.gguf' as Q6_K\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 291 tensors from /content/powershell_sentinel_llama3_f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3 8B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Meta-Llama-3\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.02 MB\n",
            "llama_model_quantize_impl: quant size  =  6282.97 MB\n",
            "\n",
            "main: quantize time = 106049.64 ms\n",
            "main:    total time = 106049.64 ms\n",
            "✅ Q6_K GGUF created successfully! Size: 6.14 GB\n",
            "   -> Backed up to: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/powershell_sentinel_llama3_q6_k.gguf\n",
            "\n",
            "--- Quantizing to Q5_K_M ---\n",
            "main: build = 6134 (be48528b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/powershell_sentinel_llama3_f16.gguf' to '/content/powershell_sentinel_llama3_q5_k_m.gguf' as Q5_K_M\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 291 tensors from /content/powershell_sentinel_llama3_f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3 8B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Meta-Llama-3\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q5_K .. size =  1002.00 MiB ->   344.44 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.02 MB\n",
            "llama_model_quantize_impl: quant size  =  5459.93 MB\n",
            "\n",
            "main: quantize time = 190773.92 ms\n",
            "main:    total time = 190773.93 ms\n",
            "✅ Q5_K_M GGUF created successfully! Size: 5.34 GB\n",
            "   -> Backed up to: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/powershell_sentinel_llama3_q5_k_m.gguf\n",
            "\n",
            "--- Quantizing to Q4_K_M ---\n",
            "main: build = 6134 (be48528b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/powershell_sentinel_llama3_f16.gguf' to '/content/powershell_sentinel_llama3_q4_k_m.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 291 tensors from /content/powershell_sentinel_llama3_f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3 8B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Meta-Llama-3\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.02 MB\n",
            "llama_model_quantize_impl: quant size  =  4685.30 MB\n",
            "\n",
            "main: quantize time = 183716.49 ms\n",
            "main:    total time = 183716.49 ms\n",
            "✅ Q4_K_M GGUF created successfully! Size: 4.58 GB\n",
            "   -> Backed up to: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/powershell_sentinel_llama3_q4_k_m.gguf\n",
            "\n",
            "--- Quantizing to Q3_K_M ---\n",
            "main: build = 6134 (be48528b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/powershell_sentinel_llama3_f16.gguf' to '/content/powershell_sentinel_llama3_q3_k_m.gguf' as Q3_K_M\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 291 tensors from /content/powershell_sentinel_llama3_f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3 8B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Meta-Llama-3\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q3_K .. size =  1002.00 MiB ->   215.27 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.02 MB\n",
            "llama_model_quantize_impl: quant size  =  3825.27 MB\n",
            "\n",
            "main: quantize time = 112508.86 ms\n",
            "main:    total time = 112508.86 ms\n",
            "✅ Q3_K_M GGUF created successfully! Size: 3.74 GB\n",
            "   -> Backed up to: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/powershell_sentinel_llama3_q3_k_m.gguf\n",
            "\n",
            "--- Quantizing to Q2_K ---\n",
            "main: build = 6134 (be48528b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/powershell_sentinel_llama3_f16.gguf' to '/content/powershell_sentinel_llama3_q2_k.gguf' as Q2_K\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 291 tensors from /content/powershell_sentinel_llama3_f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3 8B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Meta-Llama-3\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q2_K .. size =  1002.00 MiB ->   164.39 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q2_K .. size =     8.00 MiB ->     1.31 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q3_K .. size =   112.00 MiB ->    24.06 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.02 MB\n",
            "llama_model_quantize_impl: quant size  =  3024.38 MB\n",
            "\n",
            "main: quantize time = 136053.04 ms\n",
            "main:    total time = 136053.04 ms\n",
            "✅ Q2_K GGUF created successfully! Size: 2.96 GB\n",
            "   -> Backed up to: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/powershell_sentinel_llama3_q2_k.gguf\n",
            "/content\n",
            "\n",
            "\n",
            "--- FINAL SUMMARY ---\n",
            "All quantization tasks are complete. Verifying files in Google Drive:\n",
            "  - ✅ powershell_sentinel_llama3_q8_0.gguf\n",
            "  - ✅ powershell_sentinel_llama3_q6_k.gguf\n",
            "  - ✅ powershell_sentinel_llama3_q5_k_m.gguf\n",
            "  - ✅ powershell_sentinel_llama3_q4_k_m.gguf\n",
            "  - ✅ powershell_sentinel_llama3_q3_k_m.gguf\n",
            "  - ✅ powershell_sentinel_llama3_q2_k.gguf\n",
            "\n",
            "Cleaning up all temporary files and directories...\n",
            "✅ Cleanup complete. Script finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (Further/Stretch) 9. FURTHER PROMPT EXPERIMENT ---\n",
        "import json\n",
        "import os\n",
        "\n",
        "# --- Step 1: Calculate Steps for 1 Full Epoch ---\n",
        "full_clean_train_path = '/content/powershell-sentinel-main/data/generated/training_data_v0_final_clean.json'\n",
        "with open(full_clean_train_path, 'r') as f:\n",
        "    num_samples = len(json.load(f))\n",
        "\n",
        "effective_batch_size = 4\n",
        "steps_per_epoch = num_samples // effective_batch_size\n",
        "\n",
        "print(f\"Full clean training set has {num_samples} samples.\")\n",
        "print(f\"Each training run will be for 1 full epoch, which is exactly {steps_per_epoch} steps.\")\n",
        "\n",
        "# --- Step 2: Define Paths and Create Directories ---\n",
        "exp_dir = \"models/prompt_exp_1_epoch\"\n",
        "drive_backup_dir = \"/content/drive/MyDrive/PowerShell_Sentinel_Full_Epoch_Prompt_Exp\"\n",
        "results_filename = \"full_epoch_prompt_results.md\"\n",
        "local_results_path = f\"{exp_dir}/{results_filename}\"\n",
        "drive_results_path = f\"{drive_backup_dir}/{results_filename}\"\n",
        "\n",
        "!mkdir -p {exp_dir}/prompt_c\n",
        "!mkdir -p {exp_dir}/prompt_d\n",
        "!mkdir -p {exp_dir}/prompt_e\n",
        "!mkdir -p {drive_backup_dir}\n",
        "\n",
        "# --- Step 3: Run Training and Backups Sequentially ---\n",
        "\n",
        "# Run 1: Prompt C (Detailed)\n",
        "print(\"\\n--- [1/5] Starting 1-Epoch Training for Prompt C (Detailed) ---\")\n",
        "!python -m powershell_sentinel.train \\\n",
        "    --model_name meta-llama/Meta-Llama-3-8B-Instruct \\\n",
        "    --train_dataset scripts/prompt_engineering/dataset_prompt_C_Detailed.json \\\n",
        "    --preflight_train_dataset {full_clean_train_path} \\\n",
        "    --output_dir {exp_dir}/prompt_c \\\n",
        "    --test_dataset data/sets/test_set_v0.json \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --lora_rank 16 \\\n",
        "    --max_steps {steps_per_epoch}\n",
        "\n",
        "print(\"\\n--- Backing up Prompt C model to Google Drive... ---\")\n",
        "!cd {exp_dir} && zip -r {drive_backup_dir}/prompt_c_adapters.zip prompt_c\n",
        "print(\"✅ Backup complete.\")\n",
        "\n",
        "# Run 2: Prompt D (Minimalist)\n",
        "print(\"\\n--- [2/5] Starting 1-Epoch Training for Prompt D (Minimalist) ---\")\n",
        "!python -m powershell_sentinel.train \\\n",
        "    --model_name meta-llama/Meta-Llama-3-8B-Instruct \\\n",
        "    --train_dataset scripts/prompt_engineering/dataset_prompt_D_Minimalist.json \\\n",
        "    --preflight_train_dataset {full_clean_train_path} \\\n",
        "    --output_dir {exp_dir}/prompt_d \\\n",
        "    --test_dataset data/sets/test_set_v0.json \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --lora_rank 16 \\\n",
        "    --max_steps {steps_per_epoch}\n",
        "\n",
        "print(\"\\n--- Backing up Prompt D model to Google Drive... ---\")\n",
        "!cd {exp_dir} && zip -r {drive_backup_dir}/prompt_d_adapters.zip prompt_d\n",
        "print(\"✅ Backup complete.\")\n",
        "\n",
        "# Run 3: Prompt E (Role-Play Minimalist)\n",
        "print(\"\\n--- [3/5] Starting 1-Epoch Training for Prompt E (Role-Play Minimalist) ---\")\n",
        "!python -m powershell_sentinel.train \\\n",
        "    --model_name meta-llama/Meta-Llama-3-8B-Instruct \\\n",
        "    --train_dataset scripts/prompt_engineering/dataset_prompt_E_RolePlayMinimalist.json \\\n",
        "    --preflight_train_dataset {full_clean_train_path} \\\n",
        "    --output_dir {exp_dir}/prompt_e \\\n",
        "    --test_dataset data/sets/test_set_v0.json \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --lora_rank 16 \\\n",
        "    --max_steps {steps_per_epoch}\n",
        "\n",
        "print(\"\\n--- Backing up Prompt E model to Google Drive... ---\")\n",
        "!cd {exp_dir} && zip -r {drive_backup_dir}/prompt_e_adapters.zip prompt_e\n",
        "print(\"✅ Backup complete.\")\n",
        "\n",
        "# --- Step 4: Run Final Evaluation ---\n",
        "print(\"\\n--- [4/5] Starting Final Evaluation on All 3 Models ---\")\n",
        "print(\"This will take several hours. The results will be saved and then displayed below upon completion.\")\n",
        "!python -m scripts.prompt_engineering.evaluate_prompts \\\n",
        "    --base_model_path meta-llama/Meta-Llama-3-8B-Instruct \\\n",
        "    --val_path scripts/prompt_engineering/mini_val.json \\\n",
        "    --exp_dir {exp_dir} > {local_results_path}\n",
        "\n",
        "# --- Step 5: Backup and Display Final Results ---\n",
        "print(\"\\n--- [5/5] Backing up Final Results and Displaying ---\")\n",
        "!cp {local_results_path} {drive_results_path}\n",
        "print(f\"✅ Final results table backed up to: {drive_results_path}\")\n",
        "\n",
        "print(\"\\n--- Final Results Table ---\")\n",
        "!cat {local_results_path}\n",
        "\n",
        "print(\"\\n\\n✅ ✅ ✅ FULL PROMPT EXPERIMENT AND BACKUP COMPLETE ✅ ✅ ✅\")"
      ],
      "metadata": {
        "id": "b99dp_eqIXtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 10. QUALITATIVE EVALUTION OF QUANTISED MODELS ---\n",
        "# --- 1. SETUP AND DEPENDENCIES ---\n",
        "print(\"--- Step 1: Installing Dependencies ---\")\n",
        "!pip install llama-cpp-python peft -q\n",
        "print(\"✅ Dependencies installed.\")\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from llama_cpp import Llama\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "\n",
        "# --- 2. CONFIGURATION AND PATHS ---\n",
        "print(\"\\n--- Step 2: Configuring Paths and Prompts ---\")\n",
        "# Paths for models and results\n",
        "base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "drive_backup_path = \"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/final_model_adapters.zip\"\n",
        "local_restore_dir = \"/content/restored_model_temp\"\n",
        "high_precision_path = \"/content/merged_model_final\" # This will be recreated\n",
        "gguf_base_name = \"powershell_sentinel_llama3\"\n",
        "results_dir_drive = \"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/evaluation_results\"\n",
        "\n",
        "# Create the results directory on Google Drive\n",
        "os.makedirs(results_dir_drive, exist_ok=True)\n",
        "print(f\"✅ Evaluation results will be saved to: {results_dir_drive}\")\n",
        "\n",
        "# List of models to evaluate\n",
        "quantization_types = [\"Q8_0\", \"Q6_K\", \"Q5_K_M\", \"Q4_K_M\", \"Q3_K_M\", \"Q2_K\"]\n",
        "\n",
        "# Golden Dataset\n",
        "evaluation_prompts = [\n",
        "    \"Write a PowerShell command to get the 5 most memory-intensive processes on the system.\",\n",
        "    \"Create a PowerShell script that takes a directory path as input, zips its contents, and places the zip file on the Desktop.\",\n",
        "    \"In PowerShell, what is the key difference between an `Array` and an `ArrayList`?\",\n",
        "    \"Show me how to correctly add a `try-catch-finally` block to handle potential errors when trying to remove a file.\"\n",
        "]\n",
        "print(f\"✅ Golden Dataset with {len(evaluation_prompts)} prompts is ready.\")\n",
        "\n",
        "# --- 3. RECREATE THE HIGH-PRECISION BASELINE MODEL ---\n",
        "print(\"\\n--- Step 3: Recreating High-Precision Baseline Model ---\")\n",
        "# This step is necessary because the previous script's cleanup deleted the merged model.\n",
        "\n",
        "# A. Restore adapters\n",
        "print(\"Restoring fine-tuned model adapters...\")\n",
        "!rm -rf {local_restore_dir}\n",
        "!mkdir -p {local_restore_dir}\n",
        "!unzip -o -q \"{drive_backup_path}\" -d {local_restore_dir}\n",
        "adapter_path = next((os.path.join(root, d) for root, dirs, _ in os.walk(local_restore_dir) for d in dirs if \"final_checkpoint\" in d), None)\n",
        "if not adapter_path:\n",
        "    raise FileNotFoundError(\"Could not find 'final_checkpoint' directory.\")\n",
        "print(f\"Found adapter checkpoint at: {adapter_path}\")\n",
        "\n",
        "# B. Load base model in bfloat16\n",
        "print(\"\\nLoading base model in bfloat16 for a clean merge...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "print(\"✅ Base model loaded.\")\n",
        "\n",
        "# C. Merge and save\n",
        "print(\"\\nMerging PEFT adapters...\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path).merge_and_unload()\n",
        "print(f\"Saving the clean, high-precision merged model to: {high_precision_path}\")\n",
        "model.save_pretrained(high_precision_path)\n",
        "tokenizer.save_pretrained(high_precision_path)\n",
        "\n",
        "# D. Clean up GPU memory\n",
        "del model, base_model, tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"✅ Baseline model recreated and GPU memory cleaned.\")\n",
        "\n",
        "\n",
        "# --- 4. LOAD MODELS FOR EVALUATION ---\n",
        "print(\"\\n--- Step 4: Loading Models for Evaluation ---\")\n",
        "# Load the freshly recreated High-Precision Baseline Model\n",
        "print(\"Loading high-precision baseline model...\")\n",
        "model_baseline = AutoModelForCausalLM.from_pretrained(\n",
        "    high_precision_path, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
        ")\n",
        "tokenizer_baseline = AutoTokenizer.from_pretrained(high_precision_path)\n",
        "print(\"✅ High-precision model loaded.\")\n",
        "\n",
        "\n",
        "# --- 5. DEFINE GENERATION HELPERS ---\n",
        "def generate_baseline_response(prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    input_ids = tokenizer_baseline.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(model_baseline.device)\n",
        "    outputs = model_baseline.generate(\n",
        "        input_ids, max_new_tokens=512, eos_token_id=tokenizer_baseline.eos_token_id, do_sample=False\n",
        "    )\n",
        "    return tokenizer_baseline.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "def generate_quantized_response(llm_instance, prompt):\n",
        "    formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    output = llm_instance(formatted_prompt, max_tokens=512, echo=False, temperature=0.0)\n",
        "    return output['choices'][0]['text']\n",
        "\n",
        "print(\"\\n✅ Generation helper functions are ready.\")\n",
        "\n",
        "# --- 6. RUN EVALUATION, DISPLAY, AND COLLECT DATA ---\n",
        "print(\"\\n--- Step 6: Starting Full Model Evaluation ---\")\n",
        "all_evaluation_data = []\n",
        "\n",
        "for i, prompt in enumerate(evaluation_prompts):\n",
        "    display(Markdown(f\"## ➡️ Prompt {i+1}: `{prompt}`\\n---\"))\n",
        "\n",
        "    prompt_results = {\"prompt\": prompt, \"responses\": {}}\n",
        "\n",
        "    # Generate from baseline\n",
        "    print(f\"Generating baseline response for prompt {i+1}...\")\n",
        "    prompt_results[\"responses\"][\"High-Precision (f16)\"] = generate_baseline_response(prompt)\n",
        "    print(\"  -> Done.\")\n",
        "\n",
        "    # Loop through and generate from each quantized model\n",
        "    for quant_type in quantization_types:\n",
        "        gguf_path = f\"/content/{gguf_base_name}_{quant_type.lower()}.gguf\"\n",
        "        model_key = f\"Quantized ({quant_type})\"\n",
        "\n",
        "        if not os.path.exists(gguf_path):\n",
        "            prompt_results[\"responses\"][model_key] = \"SKIPPED - FILE NOT FOUND\"\n",
        "            print(f\"⚠️ Could not find {gguf_path}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Loading and generating for {quant_type}...\")\n",
        "        llm_quantized = Llama(model_path=gguf_path, n_gpu_layers=-1, n_ctx=4096, verbose=False)\n",
        "        prompt_results[\"responses\"][model_key] = generate_quantized_response(llm_quantized, prompt)\n",
        "\n",
        "        del llm_quantized\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"  -> Done. Unloaded {quant_type} to free memory.\")\n",
        "\n",
        "    all_evaluation_data.append(prompt_results)\n",
        "\n",
        "    # Display comparison table\n",
        "    markdown_table = \"| Model                  | Generated Output |\\n|------------------------|------------------|\\n\"\n",
        "    for model_name, output in prompt_results[\"responses\"].items():\n",
        "        cleaned_output = output.replace('\\n', '<br>').replace('|', '\\\\|')\n",
        "        markdown_table += f\"| **{model_name}** | <pre>{cleaned_output}</pre> |\\n\"\n",
        "    display(Markdown(markdown_table))\n",
        "\n",
        "print(\"\\n--- ✅ Evaluation Loop Complete ---\")\n",
        "\n",
        "# --- 7. PERSIST RESULTS TO GOOGLE DRIVE ---\n",
        "print(\"\\n--- Step 7: Persisting Results to Drive ---\")\n",
        "\n",
        "# Save as JSON\n",
        "json_path = os.path.join(results_dir_drive, \"evaluation_results.json\")\n",
        "with open(json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_evaluation_data, f, indent=4)\n",
        "print(f\"✅ Full results saved in JSON format to: {json_path}\")\n",
        "\n",
        "# Save as Markdown\n",
        "md_path = os.path.join(results_dir_drive, \"evaluation_report.md\")\n",
        "markdown_content = \"# PowerShell Sentinel - Model Quantization Evaluation Report\\n\\n\"\n",
        "for record in all_evaluation_data:\n",
        "    markdown_content += f\"## ➡️ Prompt: `{record['prompt']}`\\n\\n\"\n",
        "    markdown_content += \"| Model                  | Generated Output |\\n\"\n",
        "    markdown_content += \"|------------------------|------------------|\\n\"\n",
        "    for model_name, output in record['responses'].items():\n",
        "        markdown_content += f\"| **{model_name}** | ```powershell\\n{output}\\n``` |\\n\"\n",
        "    markdown_content += \"\\n---\\n\\n\"\n",
        "\n",
        "with open(md_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(markdown_content)\n",
        "print(f\"✅ Human-readable report saved in Markdown format to: {md_path}\")\n",
        "\n",
        "# --- 8. FINAL CLEANUP ---\n",
        "print(\"\\n--- Step 8: Final Cleanup ---\")\n",
        "!rm -rf {local_restore_dir}\n",
        "!rm -rf {high_precision_path}\n",
        "print(\"✅ Cleaned up temporary directories.\")\n",
        "\n",
        "print(\"\\n--- ✅✅✅ All Tasks Complete ✅✅✅ ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0d93e6295e1840ef97fe9d0311fcf354",
            "058625bcc90446ecaf1331178a84f8f0",
            "04312be319d344fdbb79f2deda76ecf4",
            "363afabbd9d24f37b3c4401f5c3edaa7",
            "8ab6bca18cc245f7b733d98fe744eb2e",
            "563be06d6b1642c88e5c23c8afa0d162",
            "a9b5728b26924d6191ef38de088f5bf0",
            "5a9e11fdd5c44dfeb970f16478263e77",
            "c6f8593f4400410abb62164a6c4989af",
            "1e575a0213084eb1aef068d382c6d7dc",
            "0baad327934e47a5aa3d6ea1f52e36c4",
            "fa0cf9f62dda4a30a2c1fa3745af42fa",
            "3d10e594c8de46819cc500b05b8f7148",
            "206e1c52566c44b398d73a70c80bf698",
            "8710bb36e0c1458583acf93cbf9b9d7b",
            "da1f49c6fd0a445ea4dfe3afbb0b88d8",
            "5ae40ed01a9f4aba96e317eceb8fc384",
            "2a983e2e90e743079117cb4c5aa0b0e6",
            "d63929bb31464ec7b012b507791cbf61",
            "ddeef9a671644eb59abebb4b81742c37",
            "7c986fd050dc4491a652538be2a48b2c",
            "81b1b211447e41f48059d52eeaa6230e"
          ]
        },
        "id": "2fkakn9yV3_J",
        "outputId": "6a2742bf-e74b-43e8-8451-cb3f01369bd4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Installing Dependencies ---\n",
            "✅ Dependencies installed.\n",
            "\n",
            "--- Step 2: Configuring Paths and Prompts ---\n",
            "✅ Evaluation results will be saved to: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/evaluation_results\n",
            "✅ Golden Dataset with 4 prompts is ready.\n",
            "\n",
            "--- Step 3: Recreating High-Precision Baseline Model ---\n",
            "Restoring fine-tuned model adapters...\n",
            "Found adapter checkpoint at: /content/restored_model_temp/content/powershell-sentinel-main/models/final_model/final_checkpoint\n",
            "\n",
            "Loading base model in bfloat16 for a clean merge...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d93e6295e1840ef97fe9d0311fcf354"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Base model loaded.\n",
            "\n",
            "Merging PEFT adapters...\n",
            "Saving the clean, high-precision merged model to: /content/merged_model_final\n",
            "✅ Baseline model recreated and GPU memory cleaned.\n",
            "\n",
            "--- Step 4: Loading Models for Evaluation ---\n",
            "Loading high-precision baseline model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa0cf9f62dda4a30a2c1fa3745af42fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ High-precision model loaded.\n",
            "\n",
            "✅ Generation helper functions are ready.\n",
            "\n",
            "--- Step 6: Starting Full Model Evaluation ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## ➡️ Prompt 1: `Write a PowerShell command to get the 5 most memory-intensive processes on the system.`\n---"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  generation_mode = GenerationMode.CONTRASTIVE_SEARCH\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  else:\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating baseline response for prompt 1...\n",
            "  -> Done.\n",
            "Loading and generating for Q8_0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
            "/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q8_0 to free memory.\n",
            "Loading and generating for Q6_K...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q6_K to free memory.\n",
            "Loading and generating for Q5_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q5_K_M to free memory.\n",
            "Loading and generating for Q4_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q4_K_M to free memory.\n",
            "Loading and generating for Q3_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q3_K_M to free memory.\n",
            "Loading and generating for Q2_K...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q2_K to free memory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Model                  | Generated Output |\n|------------------------|------------------|\n| **High-Precision (f16)** | <pre>You can use the `Get-Process` cmdlet along with the `Sort-Object` and `Select-Object` cmdlets to achieve this. Here is the PowerShell command:<br>```<br>Get-Process \\| Sort-Object -Property WorkingSet -Descending \\| Select-Object -First 5<br>```<br>Let me explain what this command does:<br><br>1. `Get-Process`: Retrieves a list of all running processes on the system.<br>2. `Sort-Object -Property WorkingSet -Descending`: Sorts the list of processes by their working set (memory usage) in descending order, so the most memory-intensive processes are at the top.<br>3. `Select-Object -First 5`: Selects the top 5 processes from the sorted list.<br><br>The `WorkingSet` property represents the amount of memory that a process is using, in bytes. By sorting by this property in descending order, we can easily identify the most memory-intensive processes.<br><br>Run this command in PowerShell to see the 5 most memory-intensive processes on your system!</pre> |\n| **Quantized (Q8_0)** | <pre>You can use the `Get-Process` cmdlet with the `-Sort` parameter to sort the processes by the `WS` property (which represents the working set size, or the amount of physical memory a process is using) in descending order, and then use the `Select-Object` cmdlet to select the top 5 results. Here is the command:<br>```<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5<br>```<br>This will output a list of the 5 processes that are using the most memory, along with their process ID, name, and working set size.<br><br>If you want to include additional information, such as the CPU usage and memory usage, you can use the `Select-Object` cmdlet to specify the properties you want to include. For example:<br>```<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5 @{Name=\"Process\";Expression={$_.ProcessName}}, @{Name=\"PID\";Expression={$_.Id}}, @{Name=\"WS (MB)\";Expression={$_.WS / 1MB -as [int]}}, @{Name=\"CPU (%)\";Expression={\"{0:N2}\" -f ($_CPU / $_Total * 100)}}, @{Name=\"Memory (%)\";Expression={\"{0:N2}\" -f ($_WS / $_WorkingSetMaximum * 100)}}<br>```<br>This will output a list of the 5 processes that are using the most memory, along with their process name, process ID, working set size, CPU usage, and memory usage as a percentage of the total available memory.</pre> |\n| **Quantized (Q6_K)** | <pre>You can use the `Get-Process` cmdlet with the `-Sort` parameter to sort the processes by the `WS` property (which represents the working set size in bytes) and the `-Top` parameter to get the top 5 processes. Here is the command:<br>```<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5<br>```<br>This will output a list of the 5 processes that are using the most memory, along with their process ID, name, and working set size.<br><br>If you want to get more detailed information about each process, you can use the `Format-Table` cmdlet to format the output as a table:<br>```<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5 \\| Format-Table -Property ID, ProcessName, WS, CPU, MemoryUsage<br>```<br>This will output a table with the process ID, name, working set size, CPU usage, and memory usage for the 5 most memory-intensive processes.</pre> |\n| **Quantized (Q5_K_M)** | <pre>You can use the `Get-Process` cmdlet with the `-Sort` parameter to sort the processes by the `WS` property (Working Set) in descending order, and then use the `Select-Object` cmdlet to select the top 5 processes. Here is the command:<br>```<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5<br>```<br>This will give you a list of the 5 processes that are using the most memory on the system.<br><br>If you want to include additional information about each process, such as the process name, ID, and CPU usage, you can use the `Format-Table` cmdlet:<br>```<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5 \\| Format-Table Name, ID, WS, CPU<br>```<br>This will give you a table with the process name, ID, working set, and CPU usage for the 5 most memory-intensive processes.</pre> |\n| **Quantized (Q4_K_M)** | <pre>You can use the `Get-Process` cmdlet with the `-Sort` parameter to sort the processes by the `WS` property (Working Set) in descending order, and then use the `Select-Object` cmdlet to select the top 5 results. Here is the command:<br>```<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5<br>```<br>This will give you a list of the 5 processes that are using the most memory on the system.<br><br>If you want to include additional information about each process, such as the process name, ID, and CPU usage, you can use the `Format-Table` cmdlet:<br>```<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5 \\| Format-Table Name, ID, WS, CPU<br>```<br>This will give you a table with the process name, ID, working set (memory usage), and CPU usage for the 5 most memory-intensive processes.</pre> |\n| **Quantized (Q3_K_M)** | <pre>You can use the `Get-Process` cmdlet to get a list of processes and the `Sort-Object` cmdlet to sort the list by the \"WS\" property, which represents the working set size of the process in bytes. The `Select-Object` cmdlet can then be used to select the top 5 processes. Here is the command:<br><br>```powershell<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5<br>```<br><br>This will give you a list of the 5 most memory-intensive processes on the system, sorted by their working set size in descending order.<br><br>If you want to get more information about each process, you can use the `Format-Table` cmdlet to format the output as a table:<br><br>```powershell<br>Get-Process \\| Sort-Object WS -Descending \\| Select-Object -First 5 \\| Format-Table -AutoSize<br>```<br><br>This will give you a table with the process ID, name, company, CPU time, memory usage, and working set size of the 5 most memory-intensive processes.</pre> |\n| **Quantized (Q2_K)** | <pre>You can use the `Get-Process` cmdlet and the `Where-Object` cmdlet to filter the processes based on the `MSRKB` property, which represents the memory usage of a process. Here's the command:<br><br>```powershell<br>Get-Process -Name * \\| Where-Object { $_.MemoryInfo_PRIVATE_PPC_CSIBARTACCKAGE -Top 5 }<br>```<br><br>This command will return the 5 most memory-intensive processes on the system.<br><br>Here's a breakdown of the command:<br><br>* `Get-Process -Name *`: This gets all processes on the system.<br>* `Where-Object`: This filters the processes based on the `MSRKB` property.<br>* `{ $_.MemoryInfo_PRIVATE_PPC_CSIBARTACCKAGE }`: This is the script block that is executed for each process. It accesses the `MSRKB` property and returns the process.<br>* `-Top 5`: This limits the output to the top 5 processes.<br><br>Please note that the `MSRKB` property is not a built-in property in PowerShell. It's a custom property that is added when you access the `MemoryInfo` property. The `MemoryInfo` property is a complex property that contains information about the memory usage of a process.</pre> |\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## ➡️ Prompt 2: `Create a PowerShell script that takes a directory path as input, zips its contents, and places the zip file on the Desktop.`\n---"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating baseline response for prompt 2...\n",
            "  -> Done.\n",
            "Loading and generating for Q8_0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q8_0 to free memory.\n",
            "Loading and generating for Q6_K...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q6_K to free memory.\n",
            "Loading and generating for Q5_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q5_K_M to free memory.\n",
            "Loading and generating for Q4_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q4_K_M to free memory.\n",
            "Loading and generating for Q3_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q3_K_M to free memory.\n",
            "Loading and generating for Q2_K...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q2_K to free memory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Model                  | Generated Output |\n|------------------------|------------------|\n| **High-Precision (f16)** | <pre>Here is a PowerShell script that does what you described:<br>```<br># Define the function<br>function Zip-Directory {<br>  param ($directoryPath)<br><br>  # Set the output file name<br>  $outputFileName = \"Desktop\\$(Get-ChildItem $directoryPath \\| Select-Object -First 1).Name.zip\"<br><br>  # Create the zip file<br>  Compress-Archive -Path $directoryPath\\* -DestinationPath $outputFileName -Force<br>}<br><br># Call the function<br>Zip-Directory -directoryPath \"C:\\Path\\To\\Directory\"<br>```<br>Let me explain how the script works:<br><br>1. The `function` keyword defines a new PowerShell function named `Zip-Directory`.<br>2. The `param` block defines a single parameter `directoryPath` that takes a directory path as input.<br>3. The script sets the output file name by getting the name of the first file in the input directory using `Get-ChildItem` and `Select-Object`, and appending `.zip` to the end.<br>4. The script uses the `Compress-Archive` cmdlet to create a zip file containing the contents of the input directory. The `-Path` parameter specifies the directory to compress, and the `-DestinationPath` parameter specifies the output file name. The `-Force` parameter forces the creation of the zip file even if it already exists.<br>5. Finally, the script calls the `Zip-Directory` function with the input directory path as an argument.<br><br>To use this script, simply save it to a file (e.g. `ZipDirectory.ps1`) and then run it in PowerShell, passing the directory path as an argument:<br>```<br>.\\ZipDirectory.ps1 -directoryPath \"C:\\Path\\To\\Directory\"<br>```<br>This will create a zip file containing the contents of the input directory and place it on the Desktop.</pre> |\n| **Quantized (Q8_0)** | <pre>Here is a PowerShell script that does what you described:<br>```<br># Define the directory path to zip<br>$directoryPath = Read-Host \"Enter the directory path to zip\"<br><br># Set the output zip file path<br>$zipFilePath = \"$env:USERPROFILE\\Desktop\\$(Get-Date -Format yyyy-MM-dd_HH-mm-ss)_zip.zip\"<br><br># Use the Compress-Archive cmdlet to zip the directory<br>Compress-Archive -Path $directoryPath -DestinationPath $zipFilePath -Update<br>```<br>Let me explain how the script works:<br><br>1. The first line uses the `Read-Host` cmdlet to prompt the user to enter the directory path to zip.<br>2. The second line sets the output zip file path by concatenating the user's desktop directory (`$env:USERPROFILE\\Desktop`) with a timestamp and the file name `_zip.zip`.<br>3. The third line uses the `Compress-Archive` cmdlet to zip the directory. The `-Path` parameter specifies the directory to zip, the `-DestinationPath` parameter specifies the output zip file path, and the `-Update` parameter updates the zip file if it already exists.<br><br>To use this script, simply save it to a file with a `.ps1` extension (e.g., `zip_directory.ps1`) and then run it in PowerShell. When you run the script, it will prompt you to enter the directory path to zip, and then it will create a zip file on your desktop with a timestamped name.<br><br>Note: Make sure to run the script as an administrator to avoid any permission issues.</pre> |\n| **Quantized (Q6_K)** | <pre>Here is a PowerShell script that does what you described:<br>```<br># Define the directory path as a variable<br>$directoryPath = Read-Host \"Enter the directory path: \"<br><br># Set the output zip file path<br>$zipFilePath = \"$env:USERPROFILE\\Desktop\\$(Get-ChildItem $directoryPath).BaseName.zip\"<br><br># Use the Compress-Archive cmdlet to zip the directory contents<br>Compress-Archive -Path $directoryPath -DestinationPath $zipFilePath -Force<br>```<br>Let me explain how the script works:<br><br>1. The first line uses the `Read-Host` cmdlet to prompt the user to enter the directory path.<br>2. The second line sets the output zip file path by concatenating the user's desktop directory (`$env:USERPROFILE\\Desktop`) with the base name of the input directory (using the `Get-ChildItem` cmdlet) and the `.zip` extension.<br>3. The third line uses the `Compress-Archive` cmdlet to zip the contents of the input directory. The `-Path` parameter specifies the input directory, the `-DestinationPath` parameter specifies the output zip file path, and the `-Force` parameter forces the overwrite of any existing file with the same name.<br><br>To use this script, simply save it to a file (e.g., `zip-directory.ps1`) and then run it in PowerShell:<br>```<br>.\\zip-directory.ps1<br>```<br>When you run the script, it will prompt you to enter the directory path. Enter the path, and the script will create a zip file on your desktop with the same name as the input directory.</pre> |\n| **Quantized (Q5_K_M)** | <pre>Here is a PowerShell script that does what you described:<br>```<br># Define the directory path as a variable<br>$directoryPath = Read-Host \"Enter the directory path: \"<br><br># Set the output file name and path<br>$outputFile = Join-Path -Path $env:USERPROFILE\\Desktop -ChildPath \"output.zip\"<br><br># Use the Compress-Archive cmdlet to zip the directory contents<br>Compress-Archive -Path $directoryPath -DestinationPath $outputFile -Force<br>```<br>Let me explain how the script works:<br><br>1. The first line uses the `Read-Host` cmdlet to prompt the user to enter the directory path. The input is stored in the `$directoryPath` variable.<br>2. The second line uses the `Join-Path` cmdlet to construct the output file name and path. In this case, we're joining the user's desktop directory (`$env:USERPROFILE\\Desktop`) with the file name \"output.zip\".<br>3. The third line uses the `Compress-Archive` cmdlet to zip the contents of the specified directory. The `-Path` parameter specifies the directory to compress, the `-DestinationPath` parameter specifies the output file path, and the `-Force` parameter forces the overwrite of any existing file with the same name.<br><br>To use this script, simply save it to a file (e.g., `zip-directory.ps1`) and then run it in PowerShell:<br>```<br>.\\zip-directory.ps1<br>```<br>When you run the script, it will prompt you to enter the directory path. Enter the path, and the script will create a zip file on your desktop with the contents of the specified directory.</pre> |\n| **Quantized (Q4_K_M)** | <pre>Here is a PowerShell script that does what you described:<br>```<br># Get the directory path from the user<br>$directoryPath = Read-Host \"Enter the directory path: \"<br><br># Set the output zip file path<br>$zipFilePath = \"$env:USERPROFILE\\Desktop\\$(Get-ChildItem $directoryPath).BaseName.zip\"<br><br># Compress the directory contents<br>Compress-Archive -Path $directoryPath -DestinationPath $zipFilePath -CompressionLevel Fast<br>```<br>Let me explain what each line does:<br><br>1. `Read-Host \"Enter the directory path: \"`: This line prompts the user to enter the directory path they want to zip.<br>2. `$directoryPath = ...`: This line assigns the user-input directory path to the `$directoryPath` variable.<br>3. `$zipFilePath = ...`: This line sets the output zip file path by combining the user's desktop directory (`$env:USERPROFILE\\Desktop`) with the base name of the input directory (`(Get-ChildItem $directoryPath).BaseName`) and a `.zip` extension.<br>4. `Compress-Archive -Path $directoryPath -DestinationPath $zipFilePath -CompressionLevel Fast`: This line uses the `Compress-Archive` cmdlet to compress the contents of the input directory to the output zip file. The `-Path` parameter specifies the input directory, the `-DestinationPath` parameter specifies the output zip file path, and the `-CompressionLevel` parameter sets the compression level to \"Fast\".<br><br>To use this script, simply copy and paste it into PowerShell, and then run it by pressing Enter. When prompted, enter the directory path you want to zip, and the script will create a zip file on your desktop with the same name as the input directory.<br><br>Note: Make sure to run this script in an elevated PowerShell session (i.e., as an administrator) to avoid any permission issues.</pre> |\n| **Quantized (Q3_K_M)** | <pre>Here is a PowerShell script that does what you described:<br>```<br>param (<br>    [string]$directoryPath<br>)<br><br># Set the destination path for the zip file<br>$destinationPath = \"$env:USERPROFILE\\Desktop\\$(Get-ChildItem $directoryPath \\| Select-Object -ExpandProperty BaseName)$(Get-ChildItem $directoryPath).Count.zip\"<br><br># Zip the directory contents<br>Compress-Archive -Path $directoryPath -DestinationPath $destinationPath -CompressionLevel Fast<br>```<br>Let me explain how the script works:<br><br>1. The `param` block defines a parameter `directoryPath` that takes a string input, which is the path to the directory you want to zip.<br>2. The `$destinationPath` variable is set to a string that combines the user's desktop directory, the base name of the input directory, and the count of files in the directory. This will be the name of the output zip file.<br>3. The `Compress-Archive` cmdlet is used to zip the contents of the input directory. The `-Path` parameter specifies the directory to zip, the `-DestinationPath` parameter specifies the output file name, and the `-CompressionLevel` parameter specifies the compression level (in this case, \"Fast\").<br>4. The script assumes that the input directory exists and is not empty. If the directory does not exist, the script will throw an error. If the directory is empty, the script will create an empty zip file.<br><br>To use this script, save it to a file (e.g. `zipDir.ps1`) and then run it from the command line, specifying the directory path as an argument:<br>```<br>.\\zipDir.ps1 C:\\Path\\To\\Directory<br>```<br>This will create a zip file on your desktop with a name like \"Directory 5.zip\" (assuming the directory contains 5 files).</pre> |\n| **Quantized (Q2_K)** | <pre>Here is a PowerShell script that does what you asked:<br>```<br># Set the directory path to zip<br>$directoryPath = 'C:\\Path\\To\\Directory'<br><br># Set the zip file name<br>$zipFileName = 'MyDirectoryZipFile.zip'<br><br># Set the destination path for the zip file<br>$destinationPath = $env:USERPROFILE\\desktop\\$zipFileName<br><br># Get the directory contents as a string<br>$directoryContents = Get-Content -Path $directoryPath -Recurse \\| Foreach {<br>    If ($_) {<br>        $_.ToString()<br>    }<br>}<br><br># Create the zip file<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br><br># Add the directory contents to the zip file<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zipFileName)<br>$zip = [System.IO.Compression.ZipFile]::Create($zip<br>```<br>I hope</pre> |\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## ➡️ Prompt 3: `In PowerShell, what is the key difference between an `Array` and an `ArrayList`?`\n---"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating baseline response for prompt 3...\n",
            "  -> Done.\n",
            "Loading and generating for Q8_0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q8_0 to free memory.\n",
            "Loading and generating for Q6_K...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q6_K to free memory.\n",
            "Loading and generating for Q5_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q5_K_M to free memory.\n",
            "Loading and generating for Q4_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q4_K_M to free memory.\n",
            "Loading and generating for Q3_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q3_K_M to free memory.\n",
            "Loading and generating for Q2_K...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q2_K to free memory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Model                  | Generated Output |\n|------------------------|------------------|\n| **High-Precision (f16)** | <pre>In PowerShell, `Array` and `ArrayList` are both collection types, but they have some key differences:<br><br>**Array**:<br>An `Array` is a fixed-size, zero-based, indexed collection of objects. Once created, its size cannot be changed. You can think of it as a traditional array in other programming languages.<br><br>Example: `$myArray = @(\"a\", \"b\", \"c\")`<br><br>**ArrayList**:<br>An `ArrayList` is a dynamic, resizable collection of objects. It can grow or shrink as elements are added or removed. It's similar to a `List` in other programming languages.<br><br>Example: `$myArrayList = [System.Collections.ArrayList]@(\"a\", \"b\", \"c\")`<br><br>Here are some key differences:<br><br>1. **Size**: An `Array` has a fixed size, while an `ArrayList` is dynamic and can grow or shrink.<br>2. **Indexing**: Both `Array` and `ArrayList` support indexing, but `ArrayList` allows you to add or remove elements at any position, while `Array` does not.<br>3. **Performance**: `Array` is generally faster and more memory-efficient than `ArrayList`, since it has a fixed size and doesn't need to manage dynamic memory allocation.<br>4. **Methods**: `ArrayList` has additional methods, such as `Add()`, `Remove()`, `Insert()`, and `Clear()`, which allow you to dynamically manage the collection. `Array` only has indexing and slicing operations.<br><br>In PowerShell, you can use the `@()` syntax to create an `Array`, while `ArrayList` requires the `[System.Collections.ArrayList]` type accelerator. However, in most cases, you can use the `@()` syntax and PowerShell will automatically create an `ArrayList` for you.<br><br>So, when to use each?<br><br>* Use `Array` when you need a fixed-size collection and don't plan to add or remove elements.<br>* Use `ArrayList` when you need a dynamic collection that can grow or shrink as elements are added or removed.</pre> |\n| **Quantized (Q8_0)** | <pre>In PowerShell, `Array` and `ArrayList` are both collection types, but they have some key differences:<br><br>**Array**:<br>An `Array` is a fixed-size, zero-based, indexed collection of objects. When you create an array, you specify the size of the array when you declare it, and you can only add or remove elements by reassigning the entire array. For example:<br>```powershell<br>$array = @(1, 2, 3, 4, 5)<br>```<br>Once created, the size of the array cannot be changed. You can access elements using their index (e.g., `$array[0]` for the first element).<br><br>**ArrayList**:<br>An `ArrayList` is a dynamic, resizable collection of objects. You don't need to specify the size when creating an `ArrayList`, and you can add or remove elements at any time using methods like `Add()` or `RemoveAt()`. For example:<br>```powershell<br>$arrayList = [System.Collections.ArrayList]@()<br>$arrayList.Add(1)<br>$arrayList.Add(2)<br>$arrayList.Add(3)<br>```<br>You can also use the `Add()` method to add multiple elements at once:<br>```powershell<br>$arrayList.AddRange(@(4, 5, 6))<br>```<br>The key differences are:<br><br>1. **Size**: Arrays have a fixed size, while `ArrayList`s are dynamic and can grow or shrink as needed.<br>2. **Indexing**: Arrays use zero-based indexing, while `ArrayList`s use a more traditional, one-based indexing (e.g., the first element is at index 1, not 0).<br>3. **Mutability**: Arrays are immutable, while `ArrayList`s are mutable, allowing you to modify the collection after creation.<br><br>In general, if you need a fixed-size collection, an array might be a better choice. If you need a dynamic collection that can grow or shrink, an `ArrayList` is a better fit.</pre> |\n| **Quantized (Q6_K)** | <pre>In PowerShell, `Array` and `ArrayList` are both collection types, but they have some key differences:<br><br>**Array**:<br>An `Array` is a fixed-size, zero-based, indexed collection of objects. Once created, its size cannot be changed. You can think of it as a traditional C-style array. When you create an array, you specify the size and type of the elements when you declare it.<br><br>Example:<br>```powershell<br>$array = New-Object Object[] 3<br>$array[0] = \"Hello\"<br>$array[1] = 42<br>$array[2] = $true<br>```<br>**ArrayList**:<br>An `ArrayList` is a dynamic, resizable collection of objects. It's similar to a .NET `List<T>` or a Java `ArrayList`. You can add, remove, or modify elements at any time. When you create an `ArrayList`, you don't specify the initial size or type of the elements.<br><br>Example:<br>```powershell<br>$arrayList = New-Object System.Collections.ArrayList<br>$arrayList.Add(\"Hello\")<br>$arrayList.Add(42)<br>$arrayList.Add($true)<br>$arrayList.Add(\"World\")<br>```<br>Key differences:<br><br>1. **Size**: An `Array` has a fixed size, while an `ArrayList` is dynamic and can grow or shrink as needed.<br>2. **Type**: An `Array` has a fixed type, while an `ArrayList` can contain objects of any type.<br>3. **Initialization**: When creating an `Array`, you specify the size and type of the elements. With an `ArrayList`, you don't specify the initial size or type.<br><br>In general, use an `Array` when you know the exact size and type of the collection at creation time, and use an `ArrayList` when you need a dynamic, resizable collection.</pre> |\n| **Quantized (Q5_K_M)** | <pre>In PowerShell, `Array` and `ArrayList` are both collection types, but they have some key differences:<br><br>**Array**:<br>An array is a fixed-size, zero-based, indexed collection of objects. When you create an array, you specify the size of the array when you declare it, and you can only store a fixed number of elements. Arrays are not dynamic, meaning you can't add or remove elements once the array is created.<br><br>Example:<br>```powershell<br>$array = New-Object Object[] 3<br>$array[0] = \"Hello\"<br>$array[1] = \"World\"<br>$array[2] = \"PowerShell\"<br>```<br>In this example, `$array` is an array with a fixed size of 3 elements.<br><br>**ArrayList**:<br>An `ArrayList` is a dynamic, resizable collection of objects. You can add or remove elements from an `ArrayList` at any time, and the collection will automatically resize itself. `ArrayList` is a .NET class that is implemented in PowerShell.<br><br>Example:<br>```powershell<br>$arrayList = New-Object System.Collections.ArrayList<br>$arrayList.Add(\"Hello\")<br>$arrayList.Add(\"World\")<br>$arrayList.Add(\"PowerShell\")<br>$arrayList.Add(\"again\")<br>```<br>In this example, `$arrayList` is an `ArrayList` that starts with an initial size of 0, and we add elements to it dynamically.<br><br>Key differences:<br><br>* **Size**: Arrays have a fixed size, while `ArrayList`s are dynamic and resizable.<br>* **Add/remove elements**: Arrays cannot be modified once created, while `ArrayList`s can be modified by adding or removing elements.<br>* **Performance**: Arrays are generally faster and more memory-efficient than `ArrayList`s, especially for small collections.<br><br>In general, use arrays when you know the exact number of elements you need to store, and use `ArrayList`s when you need a dynamic collection that can grow or shrink as needed.</pre> |\n| **Quantized (Q4_K_M)** | <pre>In PowerShell, `Array` and `ArrayList` are both data structures that can store a collection of objects, but they have some key differences:<br><br>**Array**:<br>An `Array` is a fixed-size, zero-based, indexed collection of objects. When you create an array, you specify the size of the array when you declare it, and it cannot be resized later. Arrays are useful when you know the exact number of elements you need to store.<br><br>Example:<br>```powershell<br>$array = New-Object Object[] 3<br>$array[0] = \"Hello\"<br>$array[1] = \"World\"<br>$array[2] = \"!\"<br>$array<br>```<br>Output:<br>```<br>Hello<br>World<br>!<br>```<br>**ArrayList**:<br>An `ArrayList` is a dynamic, resizable collection of objects. You don't need to specify the size when you create an `ArrayList`, and it can grow or shrink as elements are added or removed. `ArrayList` is useful when you don't know the exact number of elements you need to store or when you need to dynamically add or remove elements.<br><br>Example:<br>```powershell<br>$arrayList = New-Object System.Collections.ArrayList<br>$arrayList.Add(\"Hello\")<br>$arrayList.Add(\"World\")<br>$arrayList.Add(\"!\")<br>$arrayList<br>```<br>Output:<br>```<br>Hello<br>World<br>!<br>```<br>In PowerShell 3 and later, you can use the `@()` syntax to create an array, which is more concise and easier to read:<br>```powershell<br>$array = @(\"Hello\", \"World\", \"!\") # equivalent to New-Object Object[] 3<br>$arrayList = @() # equivalent to New-Object System.Collections.ArrayList<br>```<br>In summary:<br><br>* Use an `Array` when you know the exact number of elements and want a fixed-size collection.<br>* Use an `ArrayList` when you need a dynamic, resizable collection or don't know the exact number of elements.</pre> |\n| **Quantized (Q3_K_M)** | <pre>In PowerShell, `Array` and `ArrayList` are both collection types, but they have some key differences:<br><br>**Array**:<br><br>* An array is a fixed-size, zero-based collection of objects.<br>* It is defined using the `@()` syntax, e.g., `@(\"a\", \"b\", \"c\")`.<br>* Once created, the size of an array cannot be changed.<br>* Arrays are immutable, meaning you cannot add or remove elements after creation.<br>* Arrays are stored in memory as a contiguous block, which can be more memory-efficient.<br>* Arrays are suitable for small, fixed-size collections.<br><br>**ArrayList**:<br><br>* An ArrayList is a dynamic, resizable collection of objects.<br>* It is defined using the `[ArrayList]` type, e.g., `[ArrayList]\"a\", \"b\", \"c\"`.<br>* An ArrayList can grow or shrink dynamically as elements are added or removed.<br>* ArrayLists are mutable, meaning you can add or remove elements after creation.<br>* ArrayLists are stored in memory as a linked list, which can be less memory-efficient than arrays.<br>* ArrayLists are suitable for large, dynamic collections or collections with frequent changes.<br><br>In PowerShell, you can use the `@()` syntax to create an array, and the `[ArrayList]` type to create an ArrayList. However, in most cases, you can use the `@()` syntax and PowerShell will automatically create an ArrayList behind the scenes. This is because PowerShell 3 and later versions automatically convert arrays to ArrayLists when you use the `@()` syntax.<br><br>In summary, if you need a fixed-size, immutable collection, use an array. If you need a dynamic, resizable collection, use an ArrayList.</pre> |\n| **Quantized (Q2_K)** | <pre>In PowerShell, both `Array` and `ArrayList` are collections of objects, but they have some key differences:<br><br>**Array**: An array is a fixed-size, homogeneous collection of objects. It's similar to a traditional array in other programming languages. Once created, the array's size cannot be changed.<br><br>**ArrayList**: An ArrayList is a dynamic, heterogeneous collection of objects. It's similar to a list in other programming languages. The ArrayList's size can be changed by adding or removing elements.<br><br>Here are some key differences:<br><br>1. **Size**: An array's size is fixed, while an ArrayList's size can be changed.<br>2. **Type**: An array's elements must be of the same type, while an ArrayList's elements can be of any type.<br>3. **Mutability**: An array is immutable, while an ArrayList is mutable.<br><br>In PowerShell, you can create an array using the `New-Object` cmdlet:<br>```<br>$array = New-Object [int[]](1, 2, 3)<br>```<br>And you can create an ArrayList using the `New-Object` cmdlet:<br>```<br>$ArrayList = New-Object[System.Collections.Generic.ArrayList]::newobject([System.Collections.Generic.ArrayList]::newinstance([System.Collections.Generic.List]::newarray([System.Type]::gettype(), 0)))<br>```<br>Or, more simply:<br>```<br>$ArrayList = New-ArrayList<br>```<br>To create an ArrayList in PowerShell 3.0 and above.<br><br>In summary, if you need a fixed-size, homogeneous collection, use an array. If you need a dynamic, heterogeneous collection, use an ArrayList.</pre> |\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## ➡️ Prompt 4: `Show me how to correctly add a `try-catch-finally` block to handle potential errors when trying to remove a file.`\n---"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating baseline response for prompt 4...\n",
            "  -> Done.\n",
            "Loading and generating for Q8_0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q8_0 to free memory.\n",
            "Loading and generating for Q6_K...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q6_K to free memory.\n",
            "Loading and generating for Q5_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q5_K_M to free memory.\n",
            "Loading and generating for Q4_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q4_K_M to free memory.\n",
            "Loading and generating for Q3_K_M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q3_K_M to free memory.\n",
            "Loading and generating for Q2_K...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Done. Unloaded Q2_K to free memory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Model                  | Generated Output |\n|------------------------|------------------|\n| **High-Precision (f16)** | <pre>Here is an example of how to correctly add a `try-catch-finally` block to handle potential errors when trying to remove a file:<br>```<br>try {<br>    File file = new File(\"path/to/file.txt\");<br>    if (file.exists()) {<br>        file.delete();<br>    } else {<br>        System.out.println(\"File does not exist\");<br>    }<br>} catch (SecurityException e) {<br>    System.out.println(\"Error: \" + e.getMessage());<br>} catch (Exception e) {<br>    System.out.println(\"An unexpected error occurred: \" + e.getMessage());<br>} finally {<br>    System.out.println(\"File removal attempt complete\");<br>}<br>```<br>Let me explain what's going on:<br><br>* The `try` block is where we attempt to remove the file. We first check if the file exists using the `exists()` method, and if it does, we call the `delete()` method to try to remove it.<br>* The `catch` block is where we handle any errors that might occur during the file removal attempt. We have two separate `catch` blocks:<br>\t+ The first `catch` block catches `SecurityException` errors, which can occur if the program doesn't have the necessary permissions to delete the file.<br>\t+ The second `catch` block catches any other type of exception that might occur, and prints a generic error message.<br>* The `finally` block is where we put any code that we want to execute regardless of whether an error occurred or not. In this case, we simply print a message indicating that the file removal attempt is complete.<br><br>By using a `try-catch-finally` block, we can ensure that our program handles potential errors in a robust way, and provides useful feedback to the user if something goes wrong.</pre> |\n| **Quantized (Q8_0)** | <pre>Here is an example of how to correctly add a `try-catch-finally` block to handle potential errors when trying to remove a file:<br>```<br>try {<br>    File file = new File(\"path/to/file.txt\");<br>    if (file.exists()) {<br>        file.delete();<br>    } else {<br>        System.out.println(\"File does not exist\");<br>    }<br>} catch (SecurityException e) {<br>    System.out.println(\"Error deleting file: \" + e.getMessage());<br>} finally {<br>    System.out.println(\"File deletion attempt complete\");<br>}<br>```<br>Let me explain what's going on:<br><br>* The `try` block contains the code that may throw an exception, in this case, the `delete()` method of the `File` object.<br>* The `catch` block catches any `SecurityException` that may be thrown by the `delete()` method. You can catch other types of exceptions as well, or use a more general `Exception` catch block.<br>* The `finally` block is executed regardless of whether an exception was thrown or not. It's a good place to put any cleanup code that needs to run, such as closing resources or logging information.<br><br>In this example, if the file exists and is successfully deleted, the `finally` block will print \"File deletion attempt complete\". If the file does not exist, the `finally` block will still be executed. If a `SecurityException` is thrown while trying to delete the file, the `catch` block will handle it and print an error message, and the `finally` block will still be executed.<br><br>Note that you can also use a `try-with-resources` statement to automatically close the `File` object and handle any exceptions that may be thrown:<br>```<br>try (File file = new File(\"path/to/file.txt\")) {<br>    if (file.exists()) {<br>        file.delete();<br>    } else {<br>        System.out.println(\"File does not exist\");<br>    }<br>} catch (SecurityException e) {<br>    System.out.println(\"Error deleting file: \" + e.getMessage());<br>} finally {<br>    System.out.println(\"File deletion attempt complete\");<br>}<br>```<br>This is a more concise and efficient way to handle file operations, as it automatically closes the `File` object and handles any exceptions that may be thrown.</pre> |\n| **Quantized (Q6_K)** | <pre>Here is an example of how to correctly add a `try-catch-finally` block to handle potential errors when trying to remove a file:<br>```<br>try {<br>    File file = new File(\"path/to/file.txt\");<br>    if (file.exists()) {<br>        file.delete();<br>    } else {<br>        System.out.println(\"File does not exist\");<br>    }<br>} catch (SecurityException e) {<br>    System.out.println(\"Error deleting file: \" + e.getMessage());<br>} finally {<br>    System.out.println(\"File operation complete\");<br>}<br>```<br>Let me explain what's going on:<br><br>* The `try` block contains the code that may throw an exception, in this case, the `delete()` method of the `File` object.<br>* The `catch` block catches any `SecurityException` that may be thrown by the `delete()` method. You can catch other types of exceptions as well, or use a more general `Exception` catch block.<br>* The `finally` block is executed regardless of whether an exception was thrown or not. It's a good place to put any cleanup code that needs to run, such as closing resources or logging information.<br><br>In this example, if the file exists and is successfully deleted, the `finally` block will print \"File operation complete\". If the file does not exist, the `finally` block will still be executed and print the same message. If a `SecurityException` is thrown while trying to delete the file, the `catch` block will catch it and print an error message, and the `finally` block will still be executed.<br><br>Note that you can also use a `try-with-resources` statement to automatically close the `File` object and handle any exceptions that may be thrown:<br>```<br>try (File file = new File(\"path/to/file.txt\")) {<br>    if (file.exists()) {<br>        file.delete();<br>    } else {<br>        System.out.println(\"File does not exist\");<br>    }<br>} catch (SecurityException e) {<br>    System.out.println(\"Error deleting file: \" + e.getMessage());<br>} finally {<br>    System.out.println(\"File operation complete\");<br>}<br>```<br>This is a more concise and efficient way to handle file operations, as it automatically closes the `File` object and handles any exceptions that may be thrown.</pre> |\n| **Quantized (Q5_K_M)** | <pre>Here is an example of how to correctly add a `try-catch-finally` block to handle potential errors when trying to remove a file:<br>```<br>try {<br>    File file = new File(\"path/to/file.txt\");<br>    if (file.exists()) {<br>        file.delete();<br>    } else {<br>        System.out.println(\"File does not exist\");<br>    }<br>} catch (SecurityException e) {<br>    System.out.println(\"Error deleting file: \" + e.getMessage());<br>} finally {<br>    System.out.println(\"File operation complete\");<br>}<br>```<br>Let me explain what's going on:<br><br>* The `try` block contains the code that may throw an exception, in this case, the `delete()` method of the `File` object.<br>* The `catch` block catches any `SecurityException` that may be thrown by the `delete()` method. You can catch other types of exceptions as well, or use a more general `Exception` catch block.<br>* The `finally` block is executed regardless of whether an exception was thrown or not. It's a good place to put any cleanup code that needs to run, such as closing a resource or logging a message.<br><br>In this example, if the file exists and is successfully deleted, the `finally` block will print \"File operation complete\". If the file does not exist, the `finally` block will still be executed and print the same message. If a `SecurityException` is thrown while trying to delete the file, the `catch` block will catch it and print an error message, and the `finally` block will still be executed.<br><br>Note that you should always handle exceptions in a way that makes sense for your specific use case. In this example, I'm just printing error messages to the console, but in a real-world application, you may want to handle exceptions in a more robust way, such as by retrying the operation or logging the error.</pre> |\n| **Quantized (Q4_K_M)** | <pre>Here is an example of how to correctly add a `try-catch-finally` block to handle potential errors when trying to remove a file:<br>```<br>try {<br>    File file = new File(\"path/to/file.txt\");<br>    if (file.exists()) {<br>        file.delete();<br>    } else {<br>        System.out.println(\"File does not exist\");<br>    }<br>} catch (SecurityException e) {<br>    System.out.println(\"Error: \" + e.getMessage());<br>} finally {<br>    System.out.println(\"File operation complete\");<br>}<br>```<br>Let's break down what's happening here:<br><br>* The `try` block contains the code that may throw an exception, which is the attempt to remove the file using the `delete()` method.<br>* The `catch` block catches any `SecurityException` that may be thrown by the `delete()` method. You can catch other types of exceptions as well, or use a more general `Exception` catch block if you want to catch all types of exceptions.<br>* The `finally` block is executed regardless of whether an exception was thrown or not. In this case, it simply prints a message indicating that the file operation is complete.<br><br>Note that the `finally` block is optional, but it's a good practice to include it to ensure that any resources are cleaned up or any necessary cleanup code is executed, even if an exception is thrown.<br><br>Also, it's a good idea to check if the file exists before trying to delete it, as shown in the example. This can help prevent a `FileNotFoundException` from being thrown.<br><br>You can also use the `File.delete()` method in a more concise way using the `Optional` class:<br>```<br>Optional<File> file = Optional.ofNullable(new File(\"path/to/file.txt\"));<br>file.ifPresent(f -> f.delete());<br>```<br>This will delete the file if it exists, and do nothing if it doesn't exist.</pre> |\n| **Quantized (Q3_K_M)** | <pre>Here is an example of how to correctly add a `try-catch-finally` block to handle potential errors when trying to remove a file:<br>```<br>try {<br>    if (File.Exists(\"path/to/file.txt\")) {<br>        File.Delete(\"path/to/file.txt\");<br>    }<br>} catch (IOException ex) {<br>    Console.WriteLine(\"Error deleting file: \" + ex.Message);<br>} finally {<br>    // This block will always be executed, regardless of whether an exception was thrown<br>    // You can put any cleanup code here, such as closing a file stream<br>}<br>```<br>In this example, we're trying to delete a file at the specified path. We use the `File.Exists` method to check if the file exists before attempting to delete it. If the file exists, we use the `File.Delete` method to try to delete it.<br><br>The `try` block contains the code that may throw an exception. In this case, the `File.Delete` method may throw an `IOException` if there's an error deleting the file.<br><br>The `catch` block catches any `IOException` exceptions that are thrown by the `File.Delete` method. We can handle the exception by logging an error message to the console, for example.<br><br>The `finally` block is executed regardless of whether an exception was thrown or not. This is a good place to put any cleanup code, such as closing a file stream.<br><br>Note that you can also use a `using` statement to ensure that a file stream is properly closed, regardless of whether an exception is thrown:<br>```<br>using (var fileStream = File.Open(\"path/to/file.txt\", FileMode.Open))<br>{<br>    fileStream.Dispose();<br>}<br>```<br>This is a more modern and recommended way to handle file I/O operations in C#.</pre> |\n| **Quantized (Q2_K)** | <pre>Here is an example of how you can add a `try-catch-finally` block to handle potential errors when trying to remove a file:<br>```<br>try {<br>    // Code that might throw an exception<br>    File file = new File(\"path/to/file.txt\");<br>    file.delete();<br>} catch (IOException e) {<br>    // Error handling code<br>    System.out.println(\"Error removing file: \" + e.getMessage());<br>} finally {<br>    // Code that will always run, regardless of whether an exception was thrown<br>    System.out.println(\"File removal attempt complete\");<br>}<br>```<br>In this example, the `try` block contains the code that might throw an exception when trying to remove the file. The `catch` block handles the error, printing an error message to the console. The `finally` block is used to execute code that will always run, regardless of whether an exception was thrown.<br><br>Here's a breakdown of the different parts:<br><br>* `try`: This is the code that might throw an exception. In this case, it's the file deletion operation.<br>* `catch`: This is the error handling code. It catches the exception thrown by the `try` block and prints an error message to the console.<br>* `finally`: This is the code that will always run, regardless of whether an exception was thrown. In this case, it's a message indicating that the file removal attempt has completed.<br><br>Note that the `finally` block is optional, but it's a good practice to include it to ensure that you always execute some code after the try-catch block.<br><br>Also, make sure to handle the exception properly, as it's a good practice to handle exceptions in a way that makes sense for your application.</pre> |\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ✅ Evaluation Loop Complete ---\n",
            "\n",
            "--- Step 7: Persisting Results to Drive ---\n",
            "✅ Full results saved in JSON format to: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/evaluation_results/evaluation_results.json\n",
            "✅ Human-readable report saved in Markdown format to: /content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/evaluation_results/evaluation_report.md\n",
            "\n",
            "--- Step 8: Final Cleanup ---\n",
            "✅ Cleaned up temporary directories.\n",
            "\n",
            "--- ✅✅✅ All Tasks Complete ✅✅✅ ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (FURTHER/STRETCH) 11. QUANTITATIVE EVALUATION OF QUANTISED MODELS ---\n",
        "# ==============================================================================\n",
        "# BLOCK 1: QUANTITATIVE EVALUATION - SMOKE TEST (Definitive Version)\n",
        "# Purpose: A quick run on a small subset of data to verify the entire pipeline.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "print(\"--- Step 1: Building a Stable Environment ---\")\n",
        "# THE DEFINITIVE FIX: Manually uninstall conflicting packages and then install\n",
        "# specific, known-compatible versions to prevent ABI conflicts.\n",
        "\n",
        "# Force uninstall the default libraries to ensure a clean slate.\n",
        "!pip uninstall -y numpy scipy scikit-learn pandas numba\n",
        "print(\"\\n[Phase 1/3] Uninstalled conflicting base packages.\")\n",
        "\n",
        "# Reinstall specific, stable versions that are known to be compatible.\n",
        "# We are pinning numpy to a version before the major 2.0 release.\n",
        "!pip install numpy==1.26.4\n",
        "print(\"\\n[Phase 2/3] Installed a stable version of NumPy.\")\n",
        "\n",
        "# Now, install the rest of the stack, which will use the stable numpy.\n",
        "!pip install llama-cpp-python scikit-learn pandas -q\n",
        "print(\"\\n[Phase 3/3] Installed remaining libraries.\")\n",
        "print(\"✅ Dependencies successfully rebuilt in a stable configuration.\")\n",
        "\n",
        "\n",
        "# The Colab runtime needs to be restarted to load the new library versions correctly.\n",
        "# This will crash the current execution, which is NORMAL AND EXPECTED.\n",
        "# After it crashes, simply run this SAME CELL AGAIN from the top. The second\n",
        "# time, it will load the correct libraries and proceed.\n",
        "print(\"\\n\\n\\033[1m\\033[91m--> IMPORTANT: The Colab runtime is now restarting to load the correct libraries. <--\"\n",
        "      \"\\nAfter the restart is complete, please run this cell again to proceed with the smoke test.\\033[0m\")\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n",
        "\n",
        "# --- The rest of the script is only executed on the SECOND run after the restart ---\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "print(\"\\n--- Step 2: Configuring Paths for Smoke Test ---\")\n",
        "test_set_path = \"/content/data/sets/test_set_v0.json\"\n",
        "gguf_base_name = \"powershell_sentinel_llama3\"\n",
        "results_dir_drive = \"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/evaluation_results\"\n",
        "os.makedirs(results_dir_drive, exist_ok=True)\n",
        "quantization_types = [\"Q8_0\", \"Q6_K\", \"Q5_K_M\", \"Q4_K_M\", \"Q3_K_M\", \"Q2_K\"]\n",
        "\n",
        "# --- 3. LOAD TEST DATA ---\n",
        "print(f\"\\n--- Step 3: Loading Test Data from {test_set_path} ---\")\n",
        "if not os.path.exists(test_set_path):\n",
        "    raise FileNotFoundError(f\"CRITICAL: The test set file was not found at {test_set_path}.\")\n",
        "with open(test_set_path, 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "smoke_test_data = test_data[:15]\n",
        "print(f\"✅ Loaded {len(test_data)} total records. 🔥 SMOKE TEST will run on {len(smoke_test_data)} records.\")\n",
        "\n",
        "# --- 4. RUN QUANTITATIVE EVALUATION ---\n",
        "print(\"\\n--- Step 4: Starting Quantitative SMOKE TEST Loop ---\")\n",
        "final_results_smoke = []\n",
        "\n",
        "for quant_type in quantization_types:\n",
        "    gguf_path = f\"/content/{gguf_base_name}_{quant_type.lower()}.gguf\"\n",
        "    if not os.path.exists(gguf_path):\n",
        "        print(f\"⚠️ SKIPPING {quant_type} - GGUF file not found at {gguf_path}.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n--- Evaluating Model: {quant_type} ---\")\n",
        "    llm = Llama(model_path=gguf_path, n_gpu_layers=-1, n_ctx=4096, verbose=False)\n",
        "\n",
        "    parse_success_count = 0\n",
        "    ground_truths_intent = []\n",
        "    predictions_intent = []\n",
        "\n",
        "    for item in smoke_test_data:\n",
        "        obfuscated_command = item['input']\n",
        "        ground_truth_json = item['output']\n",
        "        formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nAnalyze the following PowerShell command and provide the output in a structured JSON format:\\n\\n```powershell\\n{obfuscated_command}\\n```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        output = llm(formatted_prompt, max_tokens=1024, echo=False, temperature=0.0)\n",
        "        response_text = output['choices'][0]['text']\n",
        "\n",
        "        try:\n",
        "            json_block = response_text[response_text.find('{') : response_text.rfind('}')+1]\n",
        "            parsed_json = json.loads(json_block)\n",
        "            parse_success_count += 1\n",
        "            ground_truths_intent.append(ground_truth_json.get(\"intent\", \"N/A\"))\n",
        "            predictions_intent.append(parsed_json.get(\"intent\", \"Error\"))\n",
        "        except (json.JSONDecodeError, IndexError):\n",
        "            ground_truths_intent.append(ground_truth_json.get(\"intent\", \"N/A\"))\n",
        "            predictions_intent.append(\"FailedToParse\")\n",
        "\n",
        "    parse_rate = (parse_success_count / len(smoke_test_data)) * 100\n",
        "    report = classification_report(ground_truths_intent, predictions_intent, output_dict=True, zero_division=0)\n",
        "\n",
        "    model_stats = {\n",
        "        \"model\": quant_type,\n",
        "        \"parse_success_rate\": f\"{parse_rate:.2f}%\",\n",
        "        \"intent_f1_score\": f\"{report['weighted avg']['f1-score']:.2f}\",\n",
        "    }\n",
        "    final_results_smoke.append(model_stats)\n",
        "    print(f\"✅ {quant_type} (Smoke Test): Parse Rate = {parse_rate:.2f}%, Intent F1 = {report['weighted avg']['f1-score']:.2f}\")\n",
        "\n",
        "    del llm\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# --- 5. DISPLAY SMOKE TEST RESULTS ---\n",
        "print(\"\\n--- Step 5: Smoke Test Summary ---\")\n",
        "df_smoke = pd.DataFrame(final_results_smoke)\n",
        "display(Markdown(\"### 🔥 Smoke Test Performance Summary\"))\n",
        "display(df_smoke)\n",
        "print(\"\\n✅ Smoke test complete. If the table above looks reasonable, you are clear to run the full evaluation script.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cEd87P_lgYhd",
        "outputId": "8f2c9226-7565-405b-ae10-2b71bb12f4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Building a Stable Environment ---\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "\u001b[33mWARNING: Skipping scipy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pandas as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping numba as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "[Phase 1/3] Uninstalled conflicting base packages.\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.19.1 requires pandas, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "pymc 5.25.1 requires pandas>=0.24.0, which is not installed.\n",
            "pymc 5.25.1 requires scipy>=1.4.1, which is not installed.\n",
            "bigframes 2.13.0 requires pandas>=1.5.3, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "arviz 0.22.0 requires pandas>=2.1.0, which is not installed.\n",
            "arviz 0.22.0 requires scipy>=1.11.0, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "pandas-gbq 0.29.2 requires pandas>=1.1.4, which is not installed.\n",
            "lightgbm 4.6.0 requires scipy, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires numba>=0.51.2, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires scipy>=1.3.1, which is not installed.\n",
            "xgboost 3.0.3 requires scipy, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "statsmodels 0.14.5 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "statsmodels 0.14.5 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "sentence-transformers 5.0.0 requires scikit-learn, which is not installed.\n",
            "sentence-transformers 5.0.0 requires scipy, which is not installed.\n",
            "pytensor 2.31.7 requires scipy<2,>=1, which is not installed.\n",
            "osqp 1.0.4 requires scipy>=0.13.2, which is not installed.\n",
            "tensorflow-decision-forests 1.12.0 requires pandas, which is not installed.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "tsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "jaxlib 0.5.3 requires scipy>=1.11.1, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "geopandas 1.1.1 requires pandas>=2.0.0, which is not installed.\n",
            "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "dask-cuda 25.6.0 requires pandas>=1.3, which is not installed.\n",
            "yfinance 0.2.65 requires pandas>=1.3.0, which is not installed.\n",
            "cmdstanpy 1.2.5 requires pandas, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "gradio 5.41.0 requires pandas<3.0,>=1.0, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\n",
            "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "clarabel 0.11.1 requires scipy, which is not installed.\n",
            "cvxpy 1.6.7 requires scipy>=1.11.0, which is not installed.\n",
            "bokeh 3.7.3 requires pandas>=1.2, which is not installed.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "jax 0.5.3 requires scipy>=1.11.1, which is not installed.\n",
            "prophet 1.1.7 requires pandas>=1.0.4, which is not installed.\n",
            "shap 0.48.0 requires numba>=0.54, which is not installed.\n",
            "shap 0.48.0 requires pandas, which is not installed.\n",
            "shap 0.48.0 requires scikit-learn, which is not installed.\n",
            "shap 0.48.0 requires scipy, which is not installed.\n",
            "holoviews 1.21.0 requires pandas>=1.3, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "distributed-ucxx-cu12 0.44.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "xarray 2025.7.1 requires pandas>=2.2, which is not installed.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, which is not installed.\n",
            "cuml-cu12 25.6.0 requires scipy>=1.8.0, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "db-dtypes 1.4.3 requires pandas>=1.5.3, which is not installed.\n",
            "fastai 2.7.19 requires pandas, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.19 requires scipy, which is not installed.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, which is not installed.\n",
            "albumentations 2.0.8 requires scipy>=1.10.0, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "diffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.4.1+cpu which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.41.0 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "c3fbe7c20c934ea68000e3dfa7760a5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase 2/3] Installed a stable version of NumPy.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "umap-learn 0.5.9.post2 requires numba>=0.51.2, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "shap 0.48.0 requires numba>=0.54, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "gradio 5.41.0 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (FURTHER/STRETCH) 11. QUANTITATIVE EVALUATION OF QUANTISED MODELS ---\n",
        "# ==============================================================================\n",
        "# BLOCK 2: QUANTITATIVE EVALUATION - FULL RUN\n",
        "# Purpose: The definitive, long-running evaluation on the entire test set.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "print(\"--- Step 1: Installing Dependencies for Full Evaluation ---\")\n",
        "# Dependencies should already be installed from the smoke test, but this ensures it.\n",
        "!pip install llama-cpp-python scikit-learn pandas -q\n",
        "print(\"✅ Dependencies installed.\")\n",
        "\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "print(\"\\n--- Step 2: Configuring Paths ---\")\n",
        "# This is the \"locked\" test set the model has never seen\n",
        "test_set_path = \"/content/data/sets/test_set_v0.json\"\n",
        "gguf_base_name = \"powershell_sentinel_llama3\"\n",
        "results_dir_drive = \"/content/drive/MyDrive/PowerShell_Sentinel_Final_Deliverable/evaluation_results\"\n",
        "os.makedirs(results_dir_drive, exist_ok=True)\n",
        "\n",
        "# List of models to evaluate\n",
        "quantization_types = [\"Q8_0\", \"Q6_K\", \"Q5_K_M\", \"Q4_K_M\", \"Q3_K_M\", \"Q2_K\"]\n",
        "\n",
        "# --- 3. LOAD TEST DATA ---\n",
        "print(f\"\\n--- Step 3: Loading Full Test Data from {test_set_path} ---\")\n",
        "if not os.path.exists(test_set_path):\n",
        "    raise FileNotFoundError(f\"CRITICAL: The test set file was not found at {test_set_path}.\")\n",
        "\n",
        "with open(test_set_path, 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "print(f\"✅ Loaded {len(test_data)} records for the full evaluation.\")\n",
        "\n",
        "# --- 4. RUN QUANTITATIVE EVALUATION ---\n",
        "print(\"\\n--- Step 4: Starting FULL Quantitative Evaluation Loop ---\")\n",
        "\n",
        "final_results = []\n",
        "\n",
        "# Loop through each quantized model\n",
        "for quant_type in quantization_types:\n",
        "    gguf_path = f\"/content/{gguf_base_name}_{quant_type.lower()}.gguf\"\n",
        "    if not os.path.exists(gguf_path):\n",
        "        print(f\"⚠️ SKIPPING {quant_type} - GGUF file not found at {gguf_path}.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n--- Evaluating Model: {quant_type} ---\")\n",
        "    llm = Llama(model_path=gguf_path, n_gpu_layers=-1, n_ctx=4096, verbose=False)\n",
        "\n",
        "    parse_success_count = 0\n",
        "    ground_truths_intent = []\n",
        "    predictions_intent = []\n",
        "\n",
        "    # Loop through each item in the FULL test data\n",
        "    for i, item in enumerate(test_data):\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"  -> Processing record {i+1} of {len(test_data)}...\")\n",
        "\n",
        "        obfuscated_command = item['input']\n",
        "        ground_truth_json = item['output']\n",
        "\n",
        "        formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nAnalyze the following PowerShell command and provide the output in a structured JSON format:\\n\\n```powershell\\n{obfuscated_command}\\n```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "        output = llm(formatted_prompt, max_tokens=1024, echo=False, temperature=0.0)\n",
        "        response_text = output['choices'][0]['text']\n",
        "\n",
        "        # Layer 2: Test JSON Parse Success\n",
        "        try:\n",
        "            json_block = response_text[response_text.find('{') : response_text.rfind('}')+1]\n",
        "            parsed_json = json.loads(json_block)\n",
        "            parse_success_count += 1\n",
        "\n",
        "            # Layer 3: Collect data for F1-Score (Intent field)\n",
        "            ground_truths_intent.append(ground_truth_json.get(\"intent\", \"N/A\"))\n",
        "            predictions_intent.append(parsed_json.get(\"intent\", \"Error\"))\n",
        "        except (json.JSONDecodeError, IndexError):\n",
        "            ground_truths_intent.append(ground_truth_json.get(\"intent\", \"N/A\"))\n",
        "            predictions_intent.append(\"FailedToParse\")\n",
        "\n",
        "    # Calculate final metrics for this model\n",
        "    parse_rate = (parse_success_count / len(test_data)) * 100\n",
        "    report = classification_report(ground_truths_intent, predictions_intent, output_dict=True, zero_division=0)\n",
        "\n",
        "    model_stats = {\n",
        "        \"model\": quant_type,\n",
        "        \"parse_success_rate\": f\"{parse_rate:.2f}%\",\n",
        "        \"intent_f1_score\": f\"{report['weighted avg']['f1-score']:.2f}\",\n",
        "        \"full_classification_report\": report\n",
        "    }\n",
        "    final_results.append(model_stats)\n",
        "\n",
        "    print(f\"✅ {quant_type} (Full Run): Parse Rate = {parse_rate:.2f}%, Intent F1 = {report['weighted avg']['f1-score']:.2f}\")\n",
        "\n",
        "    del llm\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# --- 5. PERSIST QUANTITATIVE RESULTS ---\n",
        "print(\"\\n--- Step 5: Persisting Quantitative Results to Drive ---\")\n",
        "# Save raw results to JSON for deep analysis\n",
        "json_path = os.path.join(results_dir_drive, \"quantitative_results_full.json\")\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(final_results, f, indent=4)\n",
        "print(f\"✅ Full quantitative results (with classification reports) saved to: {json_path}\")\n",
        "\n",
        "# Create and display a clean summary table with Pandas\n",
        "df_summary = pd.DataFrame(final_results)[[\"model\", \"parse_success_rate\", \"intent_f1_score\"]]\n",
        "display(Markdown(\"### 📊 Final Performance Summary\"))\n",
        "display(df_summary)\n",
        "\n",
        "# Save the summary table to a Markdown file for the dissertation\n",
        "md_report_path = os.path.join(results_dir_drive, \"quantitative_summary_report.md\")\n",
        "df_summary.to_markdown(md_report_path, index=False)\n",
        "print(f\"✅ Quantitative summary table saved to: {md_report_path}\")\n",
        "\n",
        "print(\"\\n--- ✅✅✅ All Tasks Complete ✅✅✅ ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ou0WT6y6gvWC",
        "outputId": "fd09ad54-052f-45a8-c7f7-479311c62e87"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Installing Dependencies for Full Evaluation ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 222, in iter_dependencies\n",
            "    req = Requirement(req_string.strip())\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/requirements.py\", line 36, in __init__\n",
            "    parsed = _parse_requirement(requirement_string)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 62, in parse_requirement\n",
            "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 80, in _parse_requirement\n",
            "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 124, in _parse_requirement_details\n",
            "    marker = _parse_requirement_marker(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 151, in _parse_requirement_marker\n",
            "    marker = _parse_marker(tokenizer)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
            "    expression = [_parse_marker_atom(tokenizer)]\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
            "    marker = _parse_marker_item(tokenizer)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
            "    marker_var_right = _parse_marker_var(tokenizer)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 317, in _parse_marker_var\n",
            "    return process_python_str(tokenizer.read().text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 331, in process_python_str\n",
            "    def process_python_str(python_str: str) -> Value:\n",
            "    \n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1536, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1622, in _log\n",
            "    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1582, in findCaller\n",
            "    if not _is_internal_frame(f):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 194, in _is_internal_frame\n",
            "    def _is_internal_frame(frame):\n",
            "    \n",
            "KeyboardInterrupt\n",
            "^C\n",
            "✅ Dependencies installed.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.rec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-635052061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_cpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMarkdown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_todata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from scipy._lib._array_api import (Array, array_namespace, is_lazy_array,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_result_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    xp_size, xp_result_type)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mis_lazy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403  # pyright: ignore[reportWildcardImportFromLibrary]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.rec'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziMUZVE9gzCA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}